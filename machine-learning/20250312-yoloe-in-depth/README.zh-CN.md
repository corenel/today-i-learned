
# YOLOE 深入解析

by @corenel (Yusu Pan) and LLMs

- [YOLOE 深入解析](#yoloe-深入解析)
  - [引言](#引言)
  - [创新框架](#创新框架)
  - [性能表现](#性能表现)
  - [技术演进](#技术演进)
    - [技术背景](#技术背景)
      - [YOLO-World：开启 YOLO 的开放世界之门](#yolo-world开启-yolo-的开放世界之门)
      - [YOLOv8 与 YOLOv11：YOLO 家族的持续进化](#yolov8-与-yolov11yolo-家族的持续进化)
      - [DINO 系列与 Grounding DINO：Transformer 架构的开放词汇检测标杆](#dino-系列与-grounding-dinotransformer-架构的开放词汇检测标杆)
    - [领域发展历程](#领域发展历程)
      - [传统目标检测的发展：从 R-CNN 到 YOLO 的进化史](#传统目标检测的发展从-r-cnn-到-yolo-的进化史)
      - [开放词汇目标检测的兴起](#开放词汇目标检测的兴起)
      - [开放词汇分割的发展](#开放词汇分割的发展)
  - [应用领域](#应用领域)
    - [多目标跟踪（MOT）和目标重识别（ReID）](#多目标跟踪mot和目标重识别reid)
    - [3D 视觉：从平面走向立体世界](#3d-视觉从平面走向立体世界)
    - [自动驾驶和机器人感知](#自动驾驶和机器人感知)
  - [未来趋势](#未来趋势)
    - [Transformer 架构的影响](#transformer-架构的影响)
    - [生成式多模态 AI 对目标检测的影响](#生成式多模态-ai-对目标检测的影响)
    - [未来 3-5 年目标检测趋势预测](#未来-3-5-年目标检测趋势预测)
  - [结语](#结语)

## 引言

近年来，计算机视觉领域的目标检测技术取得了突破性的进展，尤其是逐渐从传统的封闭类别识别，迈向能够适应多元复杂场景的「开放世界」目标检测与分割。不同于经典的 YOLO 模型仅能识别事先定义好的类别，如何让 YOLO 这样以实时、高效见长的模型，拓展到能识别任意物体的开放领域，已成为当前研究的核心挑战之一。

就在这样的技术背景下，2025 年 3 月正式发布的 YOLOE（"YOLOE: Real-Time Seeing Anything"），提出了一种创新的统一框架。它不仅同时支持目标检测和实例分割，更重要的是实现了文本提示、视觉提示以及无提示三种灵活模式，从而真正做到了「实时看见万物」。比如，你对它说「找到桌上的咖啡杯」，YOLOE 就能立刻定位并框出目标；甚至只需要提供一张示例图片，它也能轻松在新场景中找到相似的物品。而即使你完全不告诉它要找什么，它也能自行发现并分类图像中的任意物体，这种能力以前往往只在庞大复杂的大模型中出现，现在却在轻量快速的 YOLO 家族模型中成功落地。

本文将对 YOLOE 的核心创新、技术路线和背后的思想进行深入剖析，同时回顾 YOLO 系列（如 YOLO-World、YOLOv8 和 YOLOv11）及 Transformer 架构（如 DINO-X、Grounding DINO）等相关模型的演进历程，探讨它们如何共同推动了 YOLOE 的诞生。此外，我们还会前瞻未来 3-5 年目标检测领域的发展方向，分析 Transformer 检测器（如 RT-DETR 系列和即将到来的 YOLOv12）与 YOLOE 的竞争，以及以 Qwen2.5-VL、Moondream 为代表的生成式多模态大模型可能带来的技术革新。

## 创新框架

YOLOE 最引人注目的创新在于提出了一个高效而统一的框架，可以在单一模型内实现开放世界中的目标检测与实例分割。传统检测模型往往只针对封闭集类别，虽然也有一些模型能识别任意物体，但代价通常是显著的效率降低或需要引入庞大的语言模型。YOLOE 则巧妙地实现了文本提示、图像提示和无提示检测的统一，真正做到“一模三用”，并保持了 YOLO 系列标志性的高效性。

首先在「文本提示检测」方面，YOLOE 提出了一个颇具创意的技术——「可重参数化的区域 - 文本对齐策略」（RepRTA）。简单来说，就是通过一个辅助的小网络对大模型的文本嵌入进行精细微调，让视觉特征与文本描述无缝贴合，更妙的是，这个辅助网络在推理阶段则巧妙地将这个辅助网络的参数「融入」主模型，实现了几乎零额外推理开销。YOLOE 巧妙地“白嫖”了大型语言模型的语义知识，又避免了大模型的繁重运算。举个例子，当你告诉 YOLOE「找到沙发上的小猫」，它并不需要额外的大模型辅助，而是直接借助内置的语义对齐能力迅速完成任务。

在「视觉提示检测」模式下，YOLOE 设计了一个巧妙的「语义激活视觉提示编码器」（SAVPE）。该模块将示例图片中的信息拆分为高层语义（例如物品类别）与低层视觉激活（例如具体轮廓或纹理特征）两个分支，再将这两种信息有效融合到主干特征中。这种解耦的设计，使 YOLOE 可以用极低的开销实现精准的目标定位。试想你想找某个不常见的文玩古董，只需拍一张类似物品的图片给 YOLOE 看，它便能立刻在眼前的图像里为你精准定位类似物体的位置与轮廓，而无需重新训练或大量数据支持。

更令人称道的是，在没有任何提示的情况下，YOLOE 依然能够自主地进行「无提示开放检测」。作者巧妙提出了「惰性区域 - 提示对比策略」（LRPC），内置了庞大的类别词汇表与相应的嵌入向量，通过对比学习让模型「记住」丰富的物体概念空间，使模型能够自主匹配任意目标区域与相应的类别嵌入特征。换句话说，即使你什么也不告诉它，它也能像一个熟悉世界的「老朋友」一样迅速识别并定位任何场景中的各类物品，无需依赖额外的语言模型生成描述，从而保证了实时性和低计算成本。

综上，YOLOE 通过上述三个相辅相成的模块，真正实现只用一个模型，就能满足封闭集与开放集、多种提示方式与无提示的需求，并在速度和精度之间实现了优雅的平衡。这种设计无疑为业界和学术界提供了极佳的参考案例，也让我们对目标检测技术的下一步发展充满期待。

详细的方法探究，请参考前一篇文章 [YOLOE 论文速读](../20250311-yoloe-paper-reading/README.zh-CN.md)，于此不再赘述。

## 性能表现

得益于上述三个精心设计的模块，YOLOE 实现了令人印象深刻的「一模多能」：一个模型即可同时兼顾封闭类别与开放词汇的目标检测和实例分割，并且能灵活适应文本、图像提示以及完全无提示的应用场景，同时又能维持 YOLO 系列引以为豪的实时性能。

在具体实验中，作者在 LVIS 数据集上对 YOLOE 的零样本检测和实例分割能力进行了详细测试。例如，基于 YOLOv8 大型骨干的 YOLOE-v8-L 模型，在完全未见过相关类别数据的情况下（即零样本条件下），仅凭文本提示就实现了 35.9 AP 的检测精度，而使用图像提示则达到了 34.2 AP。这意味着，仅靠自然语言的简单描述，YOLOE 就能很好地识别从未学习过的新类别物体；而如果你给 YOLOE 一张参考图片，它也能迅速「学以致用」，在新的图像中精准定位目标。

为了更好地说明 YOLOE 的进步，我们不妨将它与之前广受关注的 YOLO-World 做个对比。YOLO-Worldv2-L 模型最高成绩约为 35.5 AP，并在 T4 GPU 上实现了 80.0 FPS 的实时性能，已属非常出色。然而，YOLOE-v8-S/M/L 模型只用了 YOLO-World-v2-S/M/L 约三分之一的训练时长，就取得了超越后者 3.5/0.2/0.4 AP 的零样本检测表现，并且在 T4 GPU 上的推理速度分别是其 1.4/1.3/1.3 倍，堪称「后来居上」。

例如，YOLOE-v8-S 的训练时间为 12.0 小时，而 YOLO-Worldv2-S 为 41.7 小时，训练成本降低约 3 倍。YOLOE-v8-S 在 T4 GPU 上的 FPS 为 305.8，iPhone 12 上为 64.3，分别是 YOLO-Worldv2-S 的 1.4 倍和 1.3 倍。

| Model          | PromptType | Params    | TrainingData       | TrainingTime | FPS(T4 / iPhone)     | AP              | AP*r*               | AP*c*           | AP*f*           |
| -------------- | ---------- | --------- | ------------------ | ------------ | -------------------- | --------------- | ------------------- | --------------- | --------------- |
| GLIP-T         | T          | 232M      | OG,Cap4M           | 1337.6h      | - / -                | 26.0            | 20.8                | 21.4            | 31.0            |
| GLIPv2-T       | T          | 232M      | OG,Cap4M           | -            | - / -                | 29.0            | -                   | -               | -               |
| GDINO-T        | T          | 172M      | OG,Cap4M           | -            | - / -                | 27.4            | 18.1                | 23.3            | 32.7            |
| DetCLIP-T      | T          | 155M      | OG                 | 250.0h       | - / -                | 34.4            | 26.9                | 33.9            | 36.3            |
| G-1.5 Edge     | T          | -         | G-20M              | -            | - / -                | 33.5            | 28.0                | 34.3            | 33.9            |
| T-Rex2         | V          | -         | O365,OI,HTCH,SA-1B | -            | - / -                | 37.4            | 29.9                | 33.9            | 41.8            |
| YWorldv2-S     | T          | 13M       | OG                 | 41.7h        | 216.4 / 48.9         | 24.4            | 17.1                | 22.5            | 27.3            |
| YWorldv2-M     | T          | 29M       | OG                 | 60.0h        | 117.9 / 34.2         | 32.4            | 28.4                | 29.6            | 35.5            |
| YWorldv2-L     | T          | 48M       | OG                 | 80.0h        | 80.0 / 22.1          | 35.5            | 25.6                | 34.6            | 38.1            |
| **YOLOE-v8-S** | T / V      | 12M / 13M | OG                 | **12.0h**    | **305.8** / **64.3** | **27.9** / 26.2 | **22.3** / 21.3     | **27.8** / 27.7 | **29.0** / 25.7 |
| **YOLOE-v8-M** | T / V      | 27M / 30M | OG                 | **17.0h**    | **156.7** / **41.7** | **32.6** / 31.0 | 26.9 / **27.0**     | **31.9** / 31.7 | **34.4** / 31.1 |
| **YOLOE-v8-L** | T / V      | 45M / 50M | OG                 | **22.5h**    | **102.5** / **27.2** | **35.9** / 34.2 | **33.2** / **33.2** | **34.8** / 34.6 | **37.3** / 34.1 |
| **YOLOE-11-S** | T / V      | 10M / 12M | OG                 | **13.0h**    | **301.2** / **73.3** | **27.5** / 26.3 | 21.4 / **22.5**     | 26.8 / **27.1** | **29.3** / 26.4 |
| **YOLOE-11-M** | T / V      | 21M / 27M | OG                 | **18.5h**    | **168.3** / **39.2** | **33.0** / 31.4 | 26.9 / **27.1**     | **32.5** / 31.9 | **34.5** / 31.7 |
| **YOLOE-11-L** | T / V      | 26M / 32M | OG                 | **23.5h**    | **130.5** / **35.1** | **35.2** / 33.7 | **29.1** / 28.1     | **35.0** / 34.6 | **36.5** / 33.8 |

此外，在零样本实例分割方面，YOLOE-v8-L 模型同样表现突出，在 LVIS 数据集上取得了 23.5 的 mask AP，达到了非常有竞争力的水准。而当作者将 YOLOE 迁移到传统目标检测任务（比如 COCO 数据集）进行全数据微调时，其性能优势更加明显：YOLOE-v8-L 模型获得了 53.0% 的检测 mAP 和 42.7% 的实例分割 mAP，分别比采用相同骨干的封闭集模型 YOLOv8-L 提升了 0.6 和 0.4 个百分点。

更值得一提的是，由于 YOLOE 已经在预训练阶段就充分「见识」了丰富的开放世界词汇，因此在进行 COCO 数据集微调时，只需传统 YOLO 模型约四分之一的训练时长就能达到更高的精度水平。这一特性堪称「省时省力」的典范，非常适合工业界快速落地新应用的需求。

以上实验结果清晰地说明了 YOLOE 在零样本泛化能力、跨数据集迁移表现以及推理速度和训练效率方面的出色表现。可以说，它既继承了 YOLO 系列的实时高效传统，又借鉴了近年来视觉语言模型的泛化优势，将实时、高效、开放词汇三者完美融合，为未来更多元的视觉应用奠定了坚实的基础。

## 技术演进

### 技术背景

YOLOE 的出现并非偶然，更不是凭空创造，而是站在了过去几年目标检测领域技术革新的基础上。从经典的 YOLO 系列模型，到近期流行的多模态开放词汇检测技术，都为 YOLOE 提供了宝贵的养分。接下来，我们将按时间脉络，分别梳理 YOLO-World 系列、YOLO 家族（YOLOv8 到 YOLOv11）、以及基于 Transformer 的检测模型（如 DINO 系列和 Grounding DINO），看它们如何一步步为 YOLOE 的诞生铺平了道路。

#### YOLO-World：开启 YOLO 的开放世界之门

YOLO-World 是腾讯 AI Lab 在 2024 年提出的一种实时开放词汇检测模型，可以看作 YOLOE 最直接的技术「先驱」。传统的 YOLO 模型效率突出，但类别总是受限于训练集。为了让 YOLO 系列也能识别新类别，YOLO-World 首次将视觉 - 语言模型（如 CLIP）的预训练思想引入 YOLO 架构中。具体来说，他们创造性地提出了「可重参数化的视觉语言路径融合网络」（RepVL-PAN），巧妙地将文本特征融合进 YOLO 的特征金字塔之中，进而通过区域 - 文本对比损失，让视觉与语言在共同空间中对齐。这样一来，YOLO-World 不仅能检测训练集中已有的类别，也能「无师自通」地识别新概念。举个形象的例子，这就像给原本只懂图片语言的 YOLO 赋予了语言的“听力”，使其能听懂用户用自然语言描述的物体概念，进而检测出图中对应的目标。

YOLO-World 不仅首次实现了这一理念，更在 LVIS 数据集中取得了 35.4 AP 的优异成绩，实时性高达 52 FPS（V100 GPU）。这为 YOLOE 提供了现实参考：即如何在不牺牲推理速度的情况下有效利用语言信息。值得一提的是，YOLO-World 随后推出的改进版 YOLO-World-v2，在训练策略上做了进一步优化，还加入了图像提示模式，这为 YOLOE 提供了进一步的实践经验。事实上，YOLOE 正是在吸收 YOLO-World 成功经验的基础上，提出了更高效的“三位一体”架构，以更低的训练开销和更好的性能超越了前者。

在 YOLO-World 系列的探索中，我们也清晰地看到了如何在保持实时性的同时扩大 YOLO 的语义范围。这一点极大地启发了 YOLOE 设计 RepRTA 模块的思路，即利用重参数化实现零开销的语言信息融合，可以说是继承了 YOLO-World 的核心创新思想。同时，YOLOE 也顺势实现了超越：仅以 YOLO-World 三分之一的训练时间，就取得了更高的零样本精度。

#### YOLOv8 与 YOLOv11：YOLO 家族的持续进化

YOLOE 之所以能快速适应开放世界的检测任务，底层技术的支撑离不开 YOLO 系列模型多年积累下的优秀基础。YOLOE 本质上基于 YOLOv8 和 YOLOv11 进行开发，因此我们有必要简单回顾 YOLO 系列最新几代模型的变化和贡献。

YOLOv8 在 2023 年初由 Ultralytics 团队发布，带来了无锚框（anchor-free）设计、C2f 主干网络、解耦头（分类和回归分支分开）等多项结构改进，使 YOLO 家族的实时检测精度得以提升。到 2024 年，Ultralytics 又进一步推出 YOLOv11，在维持检测精度的前提下，通过优化网络骨干和颈部结构，显著减少了模型参数量，进一步提高了模型在边缘设备上的部署能力。比如 YOLOv11-M 比 YOLOv8-M 参数量降低了约 22%，但检测性能却反而更好。这些持续的架构优化为 YOLOE 提供了扎实的基础检测能力。

YOLOE 正是巧妙地借用了 YOLOv8 和 YOLOv11 这些高效基础架构，并在此基础上插入了自己独特的开放词汇模块。正因为如此，YOLOE 才能在迁移到传统检测数据集时表现几乎与专用封闭集模型持平，堪称「站在巨人肩膀上的创新」。

此外，最近的 YOLOv12 更进一步，引入区域注意力模块（A²）以及高效注意力机制（FlashAttention），显著提升模型的全局感知能力和效率。这种变化也预示着未来 YOLO 系列与 Transformer 架构的融合趋势，这正好符合 YOLOE 的设计理念：将高效性与更丰富的功能统一在一起。如果这一趋势成为现实，那么 YOLO 家族与 Transformer 架构之间的界限将逐渐模糊，YOLOE 的下一代版本也可能会融合更多 Transformer 的优势，实现更高层次的通用检测能力。

#### DINO 系列与 Grounding DINO：Transformer 架构的开放词汇检测标杆

在 CNN 架构之外，以 Transformer 为核心的检测器也正在蓬勃发展。2023 年以来，由 IDEA 团队开发的 DINO 和 Grounding DINO 系列模型将 Transformer 应用到开放词汇检测领域，取得了令人瞩目的成绩。这些方法主要通过 Transformer 的注意力机制与跨模态对齐来实现图像与文本的融合，并取得了极高的零样本检测精度，例如 Grounding DINO 在 COCO 的零样本检测任务上达到了前所未有的 52.5 AP。这种性能的突破清楚地表明了 Transformer 架构在多模态融合方面的巨大优势。

尤其是最近推出的 DINO-X 模型，它不仅支持文本提示，还能够处理图像提示甚至无提示的情形，几乎实现了真正意义上的“万能识别”。DINO-X 利用超过一亿图文对的大规模训练，在识别稀有长尾物体方面表现优异，明显领先于之前的开放检测器。这种思想启发了 YOLOE 提出的 LRPC（惰性区域提示对比策略）：即在训练阶段内置丰富的类别词典与嵌入向量，让模型学习到一个通用的视觉概念空间，使得模型无需任何额外提示也能自动分类所有物体。

虽然 DINO-X 通过大量数据和 Transformer 架构实现了更高的精度，但其模型庞大、算力要求较高，这与 YOLOE 强调的高效实时性能截然不同。YOLOE 从 DINO 系列汲取灵感后，做出了自己的创新选择，用更轻量化的方式实现了高效且不依赖大型模型的开放词汇能力，从而在实际部署场景中更具优势。

综上，我们可以清楚地看到 YOLOE 的创新来源于两条技术脉络的融合：一方面，它吸取了 YOLO 系列长期积累的高效、稳定的网络设计；另一方面，它也借鉴了 YOLO-World 和 Grounding DINO 等先进的开放词汇和跨模态对齐技术。YOLOE 成功在这两者之间找到了一个恰到好处的平衡点，也为未来目标检测领域指明了一条兼具高效与开放性的发展道路。

### 领域发展历程

回顾目标检测的发展历程，有助于我们更好地理解 YOLOE 及类似工作的出现到底意味着什么。计算机视觉的目标检测技术经历了从传统的人工特征时代，到深度学习时代，再到今天开放词汇时代的跨越式进步。与此同时，「开放词汇目标检测与分割」作为新兴方向，也在最近几年迅速崛起，成为研究热点。这两个技术路径最终在 YOLOE 这一类模型中巧妙地汇聚到了一起。

#### 传统目标检测的发展：从 R-CNN 到 YOLO 的进化史

目标检测作为计算机视觉中的经典任务，在过去十多年间经历了巨大变革。早期的方法依赖人工设计特征和滑动窗口搜索，效率低下且泛化性不足。直到 2012 年，伴随深度学习的兴起，以 R-CNN 为代表的两阶段检测器（先候选框提取再分类）开始登上舞台，精度迅速提升。随后，YOLO 系列单阶段检测器的出现彻底改变了这一领域的生态。YOLOv1 提出了“一步到位”的单次回归思想，让检测的速度一下子提高到了接近实时的水平。这种「一次性看完整张图，马上输出所有物体位置」的策略，使目标检测更贴近现实应用需求，也让 YOLO 系列迅速成为工业界和学术界的宠儿。

尽管最初在准确率方面稍逊于两阶段方法，但后续 YOLOv2、YOLOv3、以及 YOLOv4 等持续改进模型结构和训练策略，使 YOLO 系列的精度逐渐逼近甚至超越了传统两阶段方法。到了 2020 年，YOLOv5、YOLOv6、YOLOv7 等模型更进一步，提出了更加丰富的优化策略，例如 YOLOv7 引入扩展 ELAN 结构后甚至一举达到了 56.8% 的 COCO mAP，在效率与精度之间达到了惊人的平衡。YOLOv8 和最新的 YOLOv11 更是在继承这一思路的基础上，进一步通过锚框设计改进、解耦头结构优化，将 YOLO 系列推到了全新的高度，为 YOLOE 后续的创新提供了理想的起点。这一系列发展为后来的 YOLOE 等新一代模型提供了一个扎实的起点，使得进一步扩展到开放世界成为可能。

#### 开放词汇目标检测的兴起

传统的检测方法虽然在效率和精度方面都取得了巨大成功，但始终有一个根本问题，即它们只能识别训练中出现过的目标类别。然而实际的场景却千变万化，不可能事先定义完所有可能出现的类别。这就催生了“开放词汇目标检测”（Open-Vocabulary Detection, OVD）的新研究方向。

这一领域真正迎来重大转折是在 CLIP 等视觉语言预训练模型出现之后。以谷歌提出的 ViLD 为代表，通过将图像检测模型与大规模图文预训练模型的知识融合，使得模型无需直接标注就能检测出训练集中未见过的物体类别。ViLD、GLIP 等方法通过图文特征对齐的预训练范式，大幅提升了未见类别的检测性能，甚至接近于有监督模型的水平。例如 GLIP 在 COCO 上实现了近 50 AP 的零样本成绩，完全颠覆了人们对传统检测模型的认知。这种范式后来被 YOLO-World、YOLOE 等实时检测器所借鉴和优化，从而使开放词汇能力不再是大型 Transformer 模型的专利，而能够被部署到高效的实时检测器中。

YOLOE 正是受益于这种技术潮流的推动，以极低的额外计算开销实现了跨模态对齐，使得实时检测器在面对未知类别时的泛化能力得到了巨大提升。这体现了从固定类别走向开放世界已经不仅是技术突破，而是视觉模型发展的必然趋势。

#### 开放词汇分割的发展

目标检测之外，开放词汇分割任务也在近年迅速兴起，尤其是在图像语义分割和实例分割领域。像谷歌的 OpenSeg 模型就采用了利用图像字幕信息学习分割掩膜的创新方法，首次实现了以任意文本描述为输入直接生成对应的分割区域。OpenSeeD 和 SAM（Segment Anything Model）的出现进一步推动了这一趋势，使得模型不仅能自由地分割任意物体，还能结合用户的输入实现交互式分割。

以 OpenSeeD 为例，它首次提出了统一训练检测与分割任务的框架，启发了 YOLOE 这种同时实现检测和实例分割的融合设计。而 SAM（Segment Anything Model）则实现了真正意义上的“即指即分”，尽管 SAM 自身不具备类别识别能力，但配合开放词汇检测模型（如 Grounding DINO）后，就可以实现完整的检测到分割一体化流程。这种技术的融合趋势在 YOLOE 身上也得到了充分体现：YOLOE 能够一次完成检测与分割任务，在开放世界中提供更加精细的目标定位能力。

未来的开放词汇分割技术发展趋势，可能将更加紧密地融合 YOLOE 这样具备多种模态提示的实时检测模型，使得图像理解模型不仅能快速地检测物体，还能精准地分割并标注出这些物体的轮廓。这种统一化趋势为后续的自动驾驶、机器人视觉感知、AR/VR 场景理解提供了一个高效而通用的视觉理解前端。

总结来看，YOLOE 正是在这样的交汇点诞生的，是当前检测与分割技术融合趋势的代表作：它不仅继承了 YOLO 系列十余年的精华，延续了高效架构；同时吸收了开放词汇检测与分割领域最新的跨模态融合思想，从视觉语言模型中获得开放识别的泛化能力。我们正处于一个目标检测从单纯技术竞争转变为多模态融合、实时高效开放能力并重的时代，而 YOLOE 正是在这一关键节点上提供了一个理想的范式。

## 应用领域

YOLOE 在技术层面的突破，不仅使研究者眼前一亮，更让应用领域的落地前景变得尤为广阔。从多目标跟踪（MOT）到机器人感知，再到自动驾驶和三维视觉任务，YOLOE 提供了一套高效且灵活的视觉解决方案，极大地推动了计算机视觉技术走进现实世界。

### 多目标跟踪（MOT）和目标重识别（ReID）

在实际应用场景中，目标检测模型不仅承担识别任务，通常还会与其他任务结合，例如多目标跟踪（MOT）和目标重识别（ReID）。当前主流的跟踪方法大多采用「先检测、后关联」的策略（tracking-by-detection），也就是先通过检测模型获得目标位置，再用跟踪算法关联同一目标在连续帧间的运动轨迹。这方面，YOLO 系列一直凭借速度和高精度受到广泛应用。例如著名的 ByteTrack 算法就使用了 YOLOX 进行前端目标检测，并配合简单而高效的关联算法（如 BYTE 或 IOU 匹配），便能在 MOT17 等基准上达到行业领先的实时跟踪表现。

YOLOE 在此基础上展现了更大的潜力。由于其开放词汇能力，跟踪系统将不再局限于传统的预设类别，而能轻松应对任意新出现的目标，从而大幅拓展跟踪系统的实用范围。举个例子，假设一架无人机需要跟踪野外搜救现场中罕见的物体，比如特定救援标志或野生动物，传统模型可能无从识别，更别提跟踪；而使用 YOLOE，只需提供参考图像或简单的文字描述，就可以立即对该未知目标实现检测和跟踪，而无需针对新目标重新训练模型。

未来，YOLOE 的视觉语义嵌入向量可以直接作为跟踪算法的输入，用于不同帧目标的匹配，例如通过余弦相似度直接实现高效的多目标跟踪，省去了单独训练 ReID 模型的麻烦，简化系统的复杂性。例如，在监控系统中，YOLOE 可以同时检测并为每个人输出独特的嵌入特征，从而实时判断不同摄像头中的人物身份，大大提升了跨摄像头的追踪效率。

此外，YOLOE 也非常适合整合进近年来流行的端到端的「联合检测与重识别」（Joint Detection and Embedding, JDE）框架中。以往，检测和重识别任务是分别训练两个模型，不仅训练成本高，推理效率也偏低。JDE 模型则巧妙地将这两个任务合并到同一个神经网络中，检测器除了输出目标位置外，还会额外学习目标外观的嵌入特征，使检测和目标身份特征提取同时完成，从而一次输出即可完成跨帧目标匹配。YOLOE 自身带有丰富的视觉语义嵌入，直接利用或者稍加微调，就能在关联不同帧的相同目标时发挥重要作用。

举个实际例子：假设一个智慧仓库内安装了多个摄像头进行货物运输跟踪，以往需要针对每个新物品专门训练一个检测和跟踪系统，而有了 YOLOE 后，只要提供一次图像提示，就能立即识别新产品并完成跟踪，这极大地简化了部署的难度。考虑到 YOLOE 本身还具备实例分割功能，这对需要精确定位和形态分析的场景尤其有用，例如机器人可以借此确定某个货品的具体位置和姿态，实现精准抓取。

### 3D 视觉：从平面走向立体世界

YOLOE 的能力并不局限在平面图像，它也具备成为 3D 感知系统重要组件的潜力。尽管 YOLOE 本身是一个 2D 检测模型，但通过简单的扩展，就能快速适配到 3D 检测与分割的应用场景中。

举例来说，如果想让 YOLOE 成为一个单目 3D 检测器，只需在原有的检测输出中增加目标深度、尺寸、姿态等三维信息的回归分支即可。实际操作时，YOLOE 可以在主干网络之外添加一个轻量的深度估计模块，实时预测场景深度图，这样每个检测框的位置就能拥有准确的 3D 坐标。这种设计能够快速部署在移动机器人、无人机等设备上，比如一辆室内巡逻机器人，通过 YOLOE 提供的二维框位置和估算出的距离，就能精确识别家具、障碍物的位置，实现自主导航。

另外一种场景则是结合多模态传感器信息，YOLOE 可以作为视觉模态与其他传感器进行融合，比如配合激光雷达或深度相机，为环境中的物体赋予准确的 3D 坐标和语义标签。或者 YOLOE 可先在视觉层面对目标物进行初步定位和分类，随后再结合激光雷达或深度传感器精确测定三维空间位置，形成高精度的障碍物地图，从而构建一个更加丰富而精准的环境感知系统。这种组合式的感知系统将为自动驾驶、服务机器人、无人机等带来更高的场景理解能力，并显著提高感知任务的鲁棒性与准确性。

近期有一种名为 Open-YOLO 3D 的工作更进一步，尝试利用多视图图像的开放词汇检测结果，自动赋予三维点云以语义标签。这种技术方向很适合将 YOLOE 作为 2D 检测组件，通过它的开放识别能力为三维重建提供准确的语义标注，比如帮助无人机在城市环境扫描时自动给楼宇、道路甚至突发的障碍物打上标签，形成实时更新的三维语义地图。

此外，YOLOE 还可与 3D 视觉任务（深度估计、立体匹配、三维重建、SLAM 等）结合实现场景理解。近期一些工作试图训练通用的 3D 视觉基础模型来统一多任务，例如 DUSt3R（Dense Unconstrained Stereo 3D Reconstruction）框架被提出用于无标定稠密立体重建，并在单目/多目深度估计和相对位姿估计等任务上取得 SOTA。DUSt3R 利用 Transformer 预测稠密点云，能够在无需相机参数的情况下从图像集合恢复 3D 结构。如果将 YOLOE 的目标检测结果与这类 3D 重建模型结合，可以实现几何与语义的联合感知。想象这样一个系统：DUSt3R 或其改进版（如 MV-DUSt3R+）持续从视频帧更新场景的 3D 点云或网格（类似 CUT3R 提出的持续更新 Transformer 3D 重建模型，可维护随时间更新的场景状态），同时 YOLOE 为新帧中出现的物体提供检测和分类。通过将 YOLOE 识别的物体嵌入到重建的 3D 场景中，我们可以得到一个带语义标签的动态场景模型。比如，在一个室内房间的重建中，YOLOE 识别出“椅子”、“桌子”、“杯子”等并在点云中标记出它们的位置；当这些物体被移动或出现新物体时，连续的 3D 感知模型 CUT3R 会更新结构，而 YOLOE 则更新语义。这种协作使机器人能够“看懂”完整场景，包括几何和语义两方面。

在立体和多视角匹配任务上，YOLOE 也可发挥作用。比如在多视图几何中，通常需要找到各视角的对应像素（特征匹配），再计算相机位姿和 3D 点。这方面 DUSt3R 已经通过 3D 点云图回归和 Transformer 匹配取得进展。YOLOE 可以提供额外的信息：它可以检测出场景中的关键目标物体（如路标、家具）充当特征锚点来帮助匹配。此外，对于预测的每个 3D 点或者局部片段，如果能赋予 YOLOE 识别的语义标签（例如这是“一辆汽车的点”或“一棵树的点”），将使 SLAM 和重定位更加鲁棒，因为语义可以作为约束。近期也有将语义引入 SLAM 的趋势，而 YOLOE 提供的开放集标签让系统可以识别并利用以前未定义的新地标物体。

### 自动驾驶和机器人感知

自动驾驶是计算机视觉落地应用最广泛的场景之一，对检测算法的实时性、泛化性和准确性要求极高。在这种情境下，YOLOE 独特的开放词汇优势可以解决传统视觉模型无法识别新奇物体的问题。

相比于传统只能识别预定义障碍物的检测器，YOLOE 则只需一个简单的文本提示，如「路面上的障碍物」，即可识别道路上的突发情况，例如掉落的行李箱、轮胎、树枝或其他意外障碍物，这对于提升驾驶安全性至关重要。

例如，假设汽车行驶中前方突然出现一只动物穿过马路，这种突发情况在传统模型中可能没有相关训练数据，导致无法准确识别，但 YOLOE 则可以借助文本提示立即作出反应，有效降低碰撞风险。与此同时，YOLOE 内置的实例分割功能更能直接为车辆提供清晰的目标轮廓，有助于精准地规划避让路径。此外，YOLOE 的实时性能意味着它可以部署在车辆的车载计算设备上，实时检测路上的行人、车辆和其他障碍物，并将结果迅速提供给路径规划系统。

再进一步，如果 YOLOE 能够结合深度估计或 3D 场景重建的信息，其检测与分割结果甚至可以直接提供目标的三维位置，获得语义点云，使得摄像头在某些情况下也能够完成激光雷达类似的任务，这在成本敏感的无人驾驶场景中具有明显优势。

机器人领域同样受益于 YOLOE 的开放性和实时性能。以工业机械臂为例，传统的视觉识别模型通常只能识别提前训练好的固定类别对象，而 YOLOE 能做到「即看即识」，只需通过文本或示例图像告诉它要抓取什么，它就能迅速在视野中定位目标并输出准确的轮廓信息，帮助机械臂快速准确地抓取物品。这种功能特别适合柔性生产线的「小批量、多品种」生产任务，让机器人无需频繁换模型，就能高效处理多样化的物料或零件。

此外，无人机巡检、家庭服务机器人也能大大获益于 YOLOE。比如无人机在野外巡逻时，能够实时识别并跟踪特殊目标（如野生动物或特定标志物）；家庭服务机器人则能快速识别日常家庭场景中的各种物品，随时响应用户指令，精准定位并操作家居物件，使机器人更加智能、更加贴近生活。

综合来看，YOLOE 的开放词汇能力、高效实时性能，以及易于与其他技术融合的特性，使其在无人驾驶、机器人领域拥有广泛的应用潜力。未来几年，随着模型性能和部署效率的进一步提升，YOLOE 很可能成为新一代实时智能感知系统的核心引擎，为更多领域带来更智慧的体验。

## 未来趋势

展望未来三到五年，目标检测领域将迎来几个显著的发展趋势：Transformer 架构将进一步渗透到实时检测领域，生成式多模态大模型（如 Qwen2.5-VL、Moondream）会为目标检测带来革命性的范式变化，而融合多任务的开放词汇检测模型也将逐渐成为主流。

### Transformer 架构的影响

YOLO 系列以卷积神经网络（CNN）高效著称，而以 DETR 为代表的 Transformer 检测器则因卓越的全局感知能力备受青睐。过去，Transformer 检测器在实时性方面一直略逊于 YOLO，但近期 RT-DETR 系列的问世成功改变了这一现状。例如，最新的 RT-DETRv3 已经实现了近似 YOLO 系列的实时速度，同时精度甚至有所超越。这说明未来的实时检测领域可能会逐渐融合 CNN 与 Transformer 的优势，形成更高效、更精准的混合型架构。

具体而言，未来几年，我们或许能看到 YOLO 系列在基础架构中引入 Transformer 模块，例如注意力机制或高效局部 Transformer，进一步提升泛化能力。试想一下，这种「YOLO+Transformer」混合模型，既能像传统 YOLO 一样高速运行，又能像 Transformer 一样处理复杂场景。YOLOE 可能正是向这个方向迈出的第一步，为未来的实时开放检测模型树立典范。

从架构的角度来看，这种趋势意味着 CNN 和 Transformer 的界限将逐渐模糊，两者的融合与互补可能成为常态。或许几年后，我们在边缘设备上部署的检测器将不再区分是 CNN 还是 Transformer，而是两者的巧妙融合，做到既高效又智能。

### 生成式多模态 AI 对目标检测的影响

生成式多模态大模型（如阿里的 Qwen2.5-VL 和 Moondream 等）也在迅速发展，它们以强大的视觉理解和语言生成能力，正在重塑人们对目标检测任务的理解和期望。这类模型不仅能识别物体，更能以自然语言描述和定位图像中的任意对象，甚至以结构化的方式（如 JSON）输出边界框和属性信息。比如当你问 Qwen2.5-VL：「图片中的红色汽车在哪？」它便会直接返回汽车的精确坐标位置，而不需要你事先训练任何专门的检测模型。

然而，目前生成式大模型虽然拥有强大的通用性和交互能力，但实时检测性能和边界框精度仍不及专用的 YOLOE 等模型。因此，短期内它们不会直接取代传统检测器，更可能的场景是与传统检测模型形成优势互补。例如，移动端可能先用 YOLOE 进行快速高效的实时检测，而云端则由大模型进行更加复杂的语义理解、结果验证或场景分析。

同时，这类生成式模型也可极大减轻标注成本，甚至合成海量的训练数据，从而提高检测器对长尾、稀有类别的表现。比如，一个新类别的数据太少无法有效训练检测器时，完全可以用大模型自动生成逼真的图像及相应标注，让 YOLOE 等检测器轻松「掌握」长尾物体的识别能力。

未来三到五年，我们可能会看到这样一个景象：生成式大模型与专用检测器相互协作，大模型负责开放式、交互式的识别与理解，专用检测器则专注于快速精准的实时感知，共同构建出更灵活、更强大的视觉认知系统。

### 未来 3-5 年目标检测趋势预测

可以预见的是，开放词汇检测将逐渐成为目标检测领域的标准模式。YOLOE 已经证明，在不增加大量计算开销的情况下，实时检测也能拥抱开放世界，今后的新模型必将继续强化这一趋势。与此同时，「多任务融合」的理念也会更加深入人心：检测、分割、跟踪甚至目标识别，将逐渐汇聚到同一个模型框架内，一次性输出完整的视觉理解结果。

此外，随着 3D 视觉技术和几何基础模型的快速崛起，目标检测领域也可能进一步向「时空感知」发展。未来的自动驾驶系统，不再是逐帧独立检测，而是通过连续的视频序列构建并持续更新场景的 3D 模型。YOLOE 这样的模型则有可能作为视觉前端，实时提供精确的目标识别与定位信息，融入到整个动态感知体系中，从而形成一种能理解空间、语义和时间变化的智能视觉系统。

另一个值得注意的趋势是「长时序、跨摄像头感知」能力的提升。试想未来城市监控系统中的智能摄像头，不仅能识别物体，更能自动跨摄像头追踪目标并进行智能检索：「请帮我找到穿蓝色夹克、背双肩包的人在过去 10 分钟内的所有位置」。这一愿景的实现需要将检测模型与跨时空跟踪模型或长时序 Transformer 紧密结合，而 YOLOE 等轻量、高效的模型将是这种技术链条中至关重要的一环。

最后，尽管未来的模型会越来越强大，计算效率与部署的实用性也绝不会被忽略。无论技术怎样演进，现实世界中的硬件约束和成本限制都决定了，高效、轻量的模型架构必定会继续保持重要地位。YOLOE 正是在这种趋势下应运而生，也势必会继续引领目标检测技术的实用性与开放性发展。

## 结语

纵观 YOLOE 的整个设计，我们不难看出，这是近年来目标检测技术与多模态视觉理解融合创新的典范之作。它不仅继承了 YOLO 系列长久以来的实时、高效特性，更巧妙地将开放词汇检测与实例分割功能融入到统一的架构中，实现了真正意义上的「实时看见万物」。无论是文本提示、图像提示，还是完全无提示的情况下，YOLOE 都能以令人满意的速度和精度提供结果。这种能力，以往通常只在复杂的大模型中才能实现，而 YOLOE 却以轻量的设计轻松做到了，堪称视觉领域的一次重要里程碑。

纵观技术演进的历史，YOLOE 并非偶然之作，而是汇聚了过去几年两个重要发展方向的精华：一方面，它得益于 YOLO 系列在架构与训练上的持续迭代，站在 YOLOv8、YOLOv11 等优秀前辈的基础上；另一方面，它汲取了 YOLO-World、Grounding DINO 等开放词汇模型在视觉 - 语言对齐、跨模态理解方面的前沿经验，从而将轻量模型带入了开放世界的新纪元。YOLOE 因此能够在零样本泛化和跨场景迁移表现中脱颖而出，同时又保持了高效的计算性能和易于部署的特点。

展望未来，目标检测领域将继续沿着「开放词汇、多任务融合」的道路前进。Transformer 架构与 YOLO 系列的边界将日益模糊，二者的优势将逐渐相互融合，形成更加通用、更加高效的新型检测器。而生成式多模态大模型的兴起，将进一步推动目标检测范式的变化，让检测任务不再仅局限于传统的框和类别预测，而是朝着更丰富的语义理解与交互式感知发展。YOLOE 以及它所代表的轻量化开放词汇检测路线，将成为实现这一愿景的重要推动力量，帮助我们构建更加智能、灵活且实用的视觉系统。

在这样一个充满机遇的时代里，相信 YOLOE 所体现的创新精神将继续激励整个领域向前迈进。从研究实验室到实际落地应用，YOLOE 所开创的实时开放视觉感知范式，必将为更多领域带来深远影响，让我们与「看见万物」的未来又靠近了一步。
