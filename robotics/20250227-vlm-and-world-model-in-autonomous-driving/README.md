# Fusion and Evolution: How VLMs and World Models are Reshaping Autonomous Driving Technology

English | [简体中文](README.zh-CN.md)

by @corenel (Yusu Pan) and OpenAI Deep Research

## Table of Contents

- [Fusion and Evolution: How VLMs and World Models are Reshaping Autonomous Driving Technology](#fusion-and-evolution-how-vlms-and-world-models-are-reshaping-autonomous-driving-technology)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Paper Overview](#paper-overview)
    - [VLM-E2E: End-to-End Driving with Multimodal Driver Attention Fusion](#vlm-e2e-end-to-end-driving-with-multimodal-driver-attention-fusion)
    - [Doe-1: Closed-Loop Autonomous Driving with Large World Model](#doe-1-closed-loop-autonomous-driving-with-large-world-model)
    - [DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](#drivevlm-the-convergence-of-autonomous-driving-and-large-vision-language-models)
  - [Comprehensive Comparison and Discussion](#comprehensive-comparison-and-discussion)
    - [Architecture Integration Methods](#architecture-integration-methods)
    - [Comparison of Innovative Highlights](#comparison-of-innovative-highlights)
    - [Comparison of Experimental Performance](#comparison-of-experimental-performance)
    - [Comparison of Limitations](#comparison-of-limitations)
    - [Comparison with Existing Literature](#comparison-with-existing-literature)
  - [Outlook: Impact on the Next 3-5 Years](#outlook-impact-on-the-next-3-5-years)
  - [References](#references)

## Introduction

Autonomous driving faces numerous challenges in complex and ever-changing traffic environments, such as understanding long-tail scenarios (rare or complex situations) and making efficient decisions. Recent research trends involve introducing Vision-Language Models (VLMs) and world models into the field of autonomous driving, enhancing vehicles' environmental understanding and planning capabilities by integrating multimodal perception and generative prediction. These integrated architectures aim to equip autonomous driving systems with high-level semantic reasoning and attention mechanisms similar to human drivers, while also being able to simulate and predict environmental changes, achieving closed-loop decision-making. This paper will delve into the application of three representative papers in the "VLM/VLA + World Model" integrated architecture: ① VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion, ② Doe-1: Closed-Loop Autonomous Driving with Large World Model, ③ DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models. We will elaborate on the specific contributions, methodological innovations, and experimental results of each paper, and compare their similarities and differences in integrating VLM and world model architectures, and evaluate their advantages and limitations. In addition, we will discuss the potential impact of these studies on the development of autonomous driving technology in the next 3-5 years, and compare and comment on their achievements with existing literature (such as DriveVLM, etc.).

## Paper Overview

### VLM-E2E: End-to-End Driving with Multimodal Driver Attention Fusion

**Specific Contributions**

VLM-E2E proposes a method to enhance end-to-end autonomous driving using vision-language models, introducing multimodal driver attention to guide decision-making. Its core contributions lie in:

1. Integrating semantic descriptions provided by pre-trained vision-language models as auxiliary information into the end-to-end driving model, explicitly injecting the semantic attention of human drivers;
2. Designing a learnable weighted fusion strategy for BEV-text features to dynamically balance the importance of visual BEV features and text features;
3. Proposing the use of text semantic refinement techniques to alleviate the hallucination problem of large models, ensuring that the introduced descriptions are accurate and relevant to the driving task. Through these innovations, VLM-E2E enables end-to-end models to learn richer feature representations and have a more human-like focus on key traffic semantics.

**Methods and Innovations**

Traditional end-to-end driving often extracts Bird’s-Eye-View (BEV) features from multi-camera images and then predicts future trajectories or control commands. However, this process often loses high-level semantic information during the 2D-to-3D conversion, making it difficult to simulate human attention. VLM-E2E utilizes the pre-trained vision-language model BLIP-2 to generate initial text descriptions of the scene from front-view camera images, focusing on key objects and events that drivers pay attention to (such as pedestrians, traffic lights, etc.). To avoid potential hallucinations from large models (e.g., elements described that do not actually exist), the authors use scene ground truth annotations and high-level driving intentions to refine and correct the generated text, making the descriptions accurate and consistent with the driving task. The refined text is then encoded into text features through the pre-trained CLIP model and aligned with visual BEV features in the same embedding space. In the fusion stage, a learnable weight is introduced to dynamically adjust the proportion of BEV features and text features. This mechanism ensures that the model neither over-relies on visual geometric information nor ignores linguistic semantic information, thereby obtaining a more comprehensive and robust environmental representation. Notably, the authors also designed specific prompts for driving tasks as input to BLIP-2 to extract "actionable attention information," filtering out irrelevant details and strengthening elements related to driving decisions. Overall, the innovation of VLM-E2E lies in the first integration of driving semantic prompts generated by pre-trained VLMs into the end-to-end planning model, endowing the model with high-level semantic reasoning ability while maintaining geometric perception accuracy. Compared to previous solutions that only used VLMs to generate trajectories and then refine them (such as DriveVLM refining VLM output trajectories with an E2E model), VLM-E2E achieves true multimodal joint training with tighter feature-level fusion.

**Experimental Results**

On the nuScenes dataset, VLM-E2E demonstrated significant performance improvements over existing methods through ablation and comparison experiments. Its improvements are reflected in three key sub-tasks of autonomous driving: perception, prediction, and planning. In perception evaluation, the BEV semantic segmentation output by the model achieved the current best IoU in three out of four categories: drivable area, lane lines, vehicles, and pedestrians; for example, compared to classic BEV segmentation methods, the IoU for vehicles and drivable areas increased to 28.17→higher and 65.97→higher, respectively (specific values reported by the authors as achieving the best in 3/4 categories). In future frame prediction (scene prediction after 2 seconds), VLM-E2E's semantic and instance segmentation accuracy also leads: compared to methods like FIERY, IoU increased by about 1.6 percentage points (36.89→38.54), and PQ increased by nearly 0.7. The results of the planning task are particularly outstanding: there are significant improvements in both long-term trajectory accuracy and safety. Taking 3-second prediction as an example, VLM-E2E's trajectory L2 error is 2.68 m, which is better than all comparison methods (slightly lower than the 2.80 m of the traditional "Vanilla" baseline). Particularly noteworthy is the Collision Rate metric: within the 2-second and 3-second prediction ranges, VLM-E2E's collision rates decreased to 0.60% and 1.17%, respectively, the lowest among all methods. In contrast, the suboptimal method ST-P3 has 0.62% at 2 seconds and 1.27% at 3 seconds, while other methods such as Vanilla have collision rates as high as 2% or more. This indicates that VLM-E2E has significantly enhanced planning robustness and safety over longer time scales. For example, although some baselines are slightly better in 1-second short prediction (ST-P3's 1-second collision rate of 0.23% is slightly lower than VLM-E2E's 0.26%), VLM-E2E continuously maintains its advantage in the 2-3 second range, and can better avoid potential collisions. The authors' ablation experiments also verified the roles of each module: removing text fusion or not performing weight balancing will lead to performance degradation, proving that semantic attention signals and dynamic fusion are indispensable for improving effectiveness. Overall, VLM-E2E outperforms past end-to-end methods with significantly lower long-distance errors and collision risks, demonstrating the value of multimodal attention fusion in complex scene driving decisions.

**Advantages and Limitations**

The advantage of VLM-E2E is that it improves the end-to-end driving model's perception of high-level semantics and key attention. By mimicking the process of human drivers paying attention to traffic signs and pedestrian dynamics, the model's decisions are more in line with human intuition, resulting in better performance in complex scenarios. As the results show, especially in situations requiring semantic understanding such as nighttime pedestrians and busy intersections, the model can better identify risks and plan responses, which is helpful for improving the safety and interpretability of autonomous driving. In addition, VLM-E2E only introduces VLM-generated text as auxiliary information during the training phase, and does not need to call large models in real-time during inference, so the inference overhead is small. This means that no additional giant model computation is needed during deployment, greatly reducing the application threshold in practice.

However, its limitations are also worth noting: First, this method relies on existing labeled data and pre-trained models for semantic refinement, requiring high-quality annotations (such as accurate 3D detection and high-level intentions) to guide text correction. This may limit the effectiveness in cases of data scarcity or inaccurate annotations. Second, the quality of the introduced text description is crucial to the training effect; although hallucination is reduced through ground truth correction, there may still be cases of incomplete descriptions (for example, the model only focuses on pedestrians but misses vehicles in the description). In addition, VLM-E2E is still an open-loop planning evaluation, i.e., directly outputting trajectories based on current observations, without feeding back and recycling prediction results. This means that it has not completely solved the closed-loop problem in the decision-making chain. The model can only "output the next step more intelligently," but there is no built-in mechanism to verify or multi-step deduce the consequences of its decisions. This point is further explored in the subsequent Doe-1 work. Finally, compared to some solutions that directly use LLMs for reasoning, the semantic information of VLM-E2E is "statically fused" into the model weights. Although inference is efficient, the flexibility is slightly inferior—if encountering new scenarios not seen in the training set, its semantic understanding ability depends on the model's existing representations, which is limited to some extent by the training distribution.

### Doe-1: Closed-Loop Autonomous Driving with Large World Model

**Specific Contributions**

Doe-1 proposes the first framework that uses a generative large world model for closed-loop end-to-end autonomous driving, realizing a unified model of perception, prediction, and planning. The main contributions are:

1. Formalizing the autonomous driving process as a "next Token generation" problem, using multimodal discrete Token sequences to simultaneously represent scene descriptions, future predictions, and driving actions. Through this representation, perception, planning, and prediction can be continuously generated in a self-regressive manner in the same Transformer model, forming a closed loop;
2. Designing a Tokenization scheme for driving: using free-text descriptions as perception output Tokens (describing the semantic scene of current observations), using image Tokens to directly generate image predictions of the next moment, and using position-sensitive discrete action Tokens to encode vehicle control commands. This scheme cleverly discretizes continuous perception and control signals, making it easy for Transformer to process;
3. Training a multimodal Transformer model (Doe-1) that can autoregressively generate a "description - action - next observation" loop. The model is verified on the nuScenes dataset to be usable for multiple tasks, including visual question answering (evaluating perception semantics), action-conditional video generation (evaluating prediction ability), and motion planning (evaluating decision quality).

In other words, Doe-1 pioneered the use of large models to generate closed-loop driving sequences, unifying traditionally separated perception, prediction, and planning modules in a single model. This is a major leap forward in existing methods of using large models for driving: some previous attempts either directly call LLMs for question answering/decision-making, or introduce VLMs for assistance but are still not closed-loop; Doe-1 truly enables the model to learn the entire process of "generating descriptions from observations, then deciding actions, and then predicting the next observation," making the model itself a driving agent with a certain degree of endogenous environmental simulation capability.

**Methods and Innovations**

Methodologically, the key to Doe-1 is the design of a unified sequence representation and closed-loop generation mechanism. First, for each time step, the environment is represented by six-way surround camera observations (provided by nuScenes) and corresponding high-level semantic descriptions. The authors use natural language to fully describe the current scene (including dynamic objects, road environment, etc.) and encode it into a string of text Tokens. Then, conditioned on these descriptions, the model generates corresponding driving action Tokens. The action is defined as the vehicle displacement (∆x, ∆y) and heading change (∆yaw) in the near future, quantized into discrete symbols in the bird's-eye view coordinate system, and the spatial meaning of the action is ensured by the position encoding strategy proposed by the authors. Finally, given the action, the model generates the next moment's observation image Token sequence, i.e., directly predicting in the image pixel space. During training, the above triad of "observation description → action → next observation" is regarded as a continuous sequence, allowing the Transformer to learn to sequentially output descriptions, actions, and the next frame in a loop, thereby closing the decision loop in time. This closed-loop autoregressive generation means that the model not only learns perception (Observation→Description), but also learns decision-making (Description→Action) and state transition (Action→Observation). During inference, the model can start from the initial observation and repeatedly generate description-action-new observation, just like a human driver continuously observing the environment, interpreting it, taking action, and then observing the new environment. Therefore, Doe-1 is essentially a multimodal sequence generation model, and its "world model" is reflected in its ability to imagine future visual scenes based on actions.

The innovations of Doe-1 include:

1. Unified multimodal Token representation: Previous works usually split perception and planning, or only use text to output high-level decisions. Doe-1 concatenates text and image Tokens, enabling the learning of the association between high-level semantics and low-level pixels in the same model, which is a pioneering attempt.
2. Closed-loop training paradigm: Unlike traditional open-loop methods (the model only outputs the next step without simulating subsequent effects), Doe-1 allows the model to directly predict the consequences of its own actions (the next frame image), which is equivalent to embedding an environment simulator, thereby building a decision-making closed loop in training. This approach is conducive to the model learning higher-order causal relationships (e.g., the cumulative effect of two consecutive actions) and interaction feedback.
3. Multi-task consistency: The authors cleverly express tasks such as visual question answering, video generation, and trajectory planning as different Prompt sequence designs for Doe-1, enabling a single model to transfer across different tasks without training a dedicated model for each task. For example: providing only the current observation as input Prompt generates scene descriptions and answers questions, using the current observation + given future action sequence as Prompt generates future image sequences, and using past observations + historical motion as Prompt generates future trajectories. This flexible task definition demonstrates the versatility of large world models.
4. Discrete action encoding and scale processing: Using grid-quantized BEV coordinate discrete actions and adding position encoding, the continuous control problem is transformed into a predictable finite vocabulary sequence. Solving this technical difficulty enables Transformer to handle trajectory output, which greatly reduces the difficulty compared to directly outputting continuous values.

**Experimental Results**

The authors comprehensively evaluated Doe-1 on the nuScenes dataset, including visual question answering (perception and semantic understanding), action-conditional video generation (prediction), and end-to-end planning (decision-making). The results show that although Doe-1 only uses front-view monocular camera images as input and training supervision mainly comes from high-level descriptions and QA, the model still exhibits competitive performance.

In the visual question answering task, Doe-1 uses the nuScenes question answering benchmark provided by OmniDrive (questions and answers generated by GPT-4) for evaluation. This task includes two categories: one is the caption generation quality of scene descriptions, and the other is the accuracy of counterfactual reasoning for hypothetical conditions. Evaluation metrics use METEOR, CIDEr, and ROUGE, commonly used in language generation, to measure the text similarity between descriptions and answers, and AP/AR to evaluate the accuracy and recall of the model's prediction of several key events. The results show that Doe-1's performance is close to or even surpasses the previous OmniDrive method: for example, in terms of description quality, Doe-1's CIDEr score is 72.8, which is significantly higher than OmniDrive-3D's 68.6; the ROUGE score of 35.6 is also higher than the latter's 32.6. For counterfactual questions (such as what would happen given a hypothetical trajectory), Doe-1's average precision AP for predicting collisions, off-road events, etc., reaches 54.5%, which is slightly improved compared to OmniDrive-3D (52.3%), and reaches 54.0% on recall AR. It is worth noting that OmniDrive-3D/2D utilizes multi-camera and even 3D point cloud information, while Doe-1 achieves comparable results with only monocular images, reflecting the powerful semantic understanding ability of large models.

In the action-conditional video generation task, Doe-1 needs to iteratively generate images of future frames based on a given sequence of control actions to simulate the observation changes after the vehicle performs the action. The evaluation metric uses Fréchet Inception Distance (FID) to measure the visual quality of the generated video relative to the real sequence. Comparing various existing world models: GAN-based DriveGAN, diffusion model-based DriveDreamer, WoVoGen, Drive-WM, GenAD, and the latest high-fidelity generation model Vista, Doe-1 achieved FID=15.9. This result is comparable to Drive-WM (15.8) and GenAD (15.4), and significantly better than early methods such as DriveGAN (73.4) and DriveDreamer (52.6). Although it failed to surpass the current best Vista (FID=6.9), considering that Vista and others are diffusion models specifically optimized for image generation, and Doe-1 takes into account multiple tasks in a unified model, it is already remarkable that its image prediction quality can approach dedicated models. More importantly, from the generated video sequences, Doe-1 not only ensures image quality, but also maintains the consistency of 3D structures in the scene and coordination with given actions. The visualizations provided by the authors show that Doe-1 can generate scene changes consistent with actual driving, such as the change in road perspective when the vehicle turns, and the shortening of the distance to the vehicle in front when accelerating, proving that the model has indeed learned physically reasonable world simulation.

In the end-to-end motion planning evaluation, the authors used standard trajectory L2 error and collision rate metrics to statistically analyze the position error and collision status after 1 second, 2 seconds, and 3 seconds of prediction. Doe-1 is trained using only camera images and high-level QA supervision (without using maps, detection, and other additional information), and its planning performance reaches a level comparable to various existing methods. For example, compared to the purely visual ST-P3 method, Doe-1 has a lower 3-second average L2 error (2.11 m vs 2.90 m), and the 3-second collision rate is also close to ST-P3 (approximately 1.19% vs 1.27%). However, compared to the latest methods that integrate multimodal information, Doe-1 has a slight gap in open-loop evaluation: for example, the latest GenAD uses camera plus map and detection supervision, with a 3-second L2 of only 0.91 m and a collision rate of 0.43%; while Doe-1's corresponding indicators are 1.26 m and 0.53%. Nevertheless, considering that Doe-1 does not rely on LiDAR or high-definition maps, the average error of 1.26 m it achieves is still in a good range (about the length of a vehicle body), and the average collision rate of 0.53% is also comparable to many traditional methods. The authors further evaluated Doe-1 using the closed-loop simulation evaluation method in the VAD paper (marked as †), and found that the model performance has significantly improved: Doe-1's 3-second L2 error in closed-loop evaluation decreased to 1.07 m, and the collision rate is only 0.47%. This improvement indicates that in the process of cyclically rolling simulation of its own decisions, the model may be able to correct some errors and make the trajectory closer to an executable and safe range. However, even so, Doe-1's planning has not surpassed the best traditional modular methods. For example, the OmniDrive framework, which combines multiple sensors and rich supervision, has a 3-second L2 of only 0.55 m and a collision rate of 0.30%; another method, GenAD (†), achieves excellent levels of 0.78 m and 0.19%. Overall, Doe-1, as a monocular closed-loop model, is slightly inferior to SOTA with multimodal sensor fusion in terms of planning accuracy, but it has already demonstrated decision-making capabilities approaching human driving levels: the model often outputs smooth and rule-abiding trajectories, and avoids collisions in most scenarios. More commendably, it can also explain the environment and predict consequences (through descriptions and generating future images), which traditional planning models cannot do.

**Advantages and Limitations**

The main advantages of Doe-1 are unity and closed-loop nature. One model can complete the three major tasks of perception, decision-making, and prediction, eliminating the need for complex pipeline integration and avoiding cascade errors and inconsistencies between modules. This "unified" architecture enables the model's understanding of the environment and inference of actions to share the same representation, thus enabling the learning of higher-order interaction relationships (for example, the model learns the association between the description "running a red light" and subsequent collisions). Closed-loop generation gives the model "introspection" ability: while generating plans, it also simulates the results of the plan, which helps the model choose safer actions (just as human driving would anticipate "will I get into trouble if I do this?"). The addition of question answering and counterfactual reasoning in the experiment further improves the model's mastery of causal relationships. In addition, Doe-1's multi-task capability means higher data utilization and scalability—training incorporates signals such as descriptions, QA, and planning, giving the model rich knowledge and enabling it to adapt to new tasks in a "multi-skilled" manner (such as answering passenger questions, reporting road conditions, etc.) by simply adjusting input prompts without retraining. This foreshadows the potential of future autonomous driving large models as general intelligent agents.

Of course, Doe-1 also has limitations and challenges: First, model scale and computation are obstacles to practical application. The authors used a pre-trained 7 billion parameter model (expanded visual part based on Lumina-mGPT 7B) and performed several rounds of fine-tuning training, which has high requirements for computing power and data. During inference, autoregressively generating text and image Tokens is also time-consuming, and it is currently difficult to run the complete generation cycle in real time on vehicles. Second, sensor information is limited: Doe-1 mainly uses RGB cameras and does not fully utilize information from LiDAR, vehicle speedometers, etc. Although multi-camera images can provide surrounding views, the lack of depth and speed information may cause the model's estimation of distance and motion to be less accurate than LiDAR-based methods. Third, model performance needs to be improved: judging from the planning results, Doe-1 still has gaps in accuracy and safety compared to the industry best. Especially in complex scenarios, such as rainy nights and multi-vehicle interactions, the model may have insufficient descriptions or prediction deviations. In addition, the hallucination and cumulative error problems inherent in generative models still exist: if the model generates a slightly deviated image or description in a certain step, subsequent steps may amplify the error. Although the authors alleviated the hallucination of LLM through training, when simulating for a long time, the accumulation of small errors may lead to drifting away from the real environment. This limits the duration of closed-loop simulation, and currently it may only be able to reliably simulate for several seconds to tens of seconds. Finally, Doe-1, as a pioneering work, provides a paradigm for achieving complete closed-loop, but how to combine it with reality needs further exploration. Its output image prediction is currently mainly used for evaluation or providing visualization, and has not been used for control feedback; in real vehicles, to close the loop, the generated frames need to be connected to vehicle sensors and handle unpredictable noise and emergencies in reality. In summary, Doe-1 proves the great potential of large generative models in autonomous driving, but before moving towards engineering applications, it is necessary to continue to improve in terms of model efficiency, multimodal fusion, and safety and reliability.

### DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models

**Specific Contributions**

The DriveVLM work directly introduces large vision-language models into autonomous driving systems, aiming to enhance environmental understanding and planning decisions through advanced semantic reasoning. Its contributions are reflected in:

1. Proposing the DriveVLM framework, which uses pre-trained Vision-Language Models (such as Qwen-VL, etc.) for understanding and decision-making in autonomous driving scenarios, constructing a combination of scene description, scene analysis, and hierarchical planning inference modules to improve the system's ability to handle complex scenarios;
2. Addressing the shortcomings of VLMs in spatial reasoning accuracy and computational burden, introducing the DriveVLM-Dual hybrid system: combining the large model inference of DriveVLM with traditional autonomous driving perception and planning pipelines to form a hybrid architecture of complementary advantages. Specifically, the Dual system retains classic perception modules (3D detection, tracking, maps, etc.) to provide accurate low-level obstacle information, while utilizing VLM modules for high-level semantic understanding and decision intention derivation, thereby obtaining both the intelligent reasoning ability of large models and ensuring real-time performance and accuracy.
3. The authors built a multimodal dataset (SUP-AD) for scene understanding and planning, and designed evaluation metrics for scene description and meta-action planning to train and evaluate DriveVLM.
4. DriveVLM-Dual has been deployed and verified in actual road tests, successfully running on real vehicles, proving its effectiveness and potential feasibility in real-world scenarios. In summary, DriveVLM is one of the first works to close-loop introduce large-scale vision-language models into real autonomous driving, and is pioneering in both methodology and system.

**Methods and Innovations**

The methodological feature of DriveVLM is to allow pre-trained large models to participate in the decision-making process, while solving the difficulty of large models directly controlling vehicles through architectural design. Its workflow is usually as follows:

1. First, sensor data such as cameras is input to the traditional perception module to obtain information such as detected vehicle and pedestrian lists, road topology, etc. Then, these perception results are transformed into abstract descriptions of the scene, for example, summarized in text form: "There is construction 100 meters ahead, occupying the left two lanes, and pedestrians are crossing the road on the right," etc. DriveVLM uses a fine-tuned large vision-language model (such as Qwen-VL 7B parameter model) to generate such scene descriptions and analyses. Specifically, the authors encode visual features (such as multi-frame images) and input them into the VLM together with human-designed prompts, allowing it to output natural language descriptions and scene analyses of the scene. "Scene analysis" here usually refers to the refinement and reasoning of key information, such as identifying risk points in the current scene, intention interpretation, etc.
2. Next, DriveVLM introduces the idea of hierarchical planning: based on the understanding of the scene, the VLM first outputs a sequence of high-level meta-actions and decision descriptions, and then the traditional planning module refines these meta-actions into specific trajectories and controls. Meta-actions can be understood as short-term strategy commands, such as "decelerate," "change lanes to the right," "keep going straight," etc. The authors pre-defined 17 types of common driving actions for the model to choose and combine. These meta-action sequences are more in line with the LLM's expertise mode—discrete language forms—than continuous coordinates, and it is also easier to integrate safety rules (such as limiting a certain action sequence to represent conservative or aggressive driving). DriveVLM's large model will plan step by step: first generate a series of meta-actions and corresponding reason descriptions based on the scene, and then the traditional control module (or a lightweight data-driven model) will convert these into a series of spatial coordinates or paths for the vehicle to execute. This two-level planning is equivalent to decomposing the complex planning problem into two stages: "what to do" (meta-action sequence, given by VLM) and "how to do it" (trajectory implementation, completed by algorithm). Since the traditional module checks in the second stage, even if the LLM has slight irrationality in the action sequence, it can be filtered or adjusted to ensure the feasibility of the final trajectory. This achieves complementary advantages of semantic reasoning and precise control.

DriveVLM-Dual further innovates in system architecture: it does not completely replace the traditional stack with end-to-end learning, but integrates two sets of systems. The traditional module continues to provide basic perception (ensuring detection accuracy and timeliness), while the large model module provides high-level decision-making suggestions as a "wisdom brain." The final action executed by the vehicle is jointly determined by both: if the large model's suggestion conflicts with traditional planning, the system can arbitrate or choose a conservative solution. This design takes into account the current computational latency of VLM. The Dual mode allows the vehicle to still move forward with classic planning every frame, while the VLM provides strategic guidance at a lower frequency, which is equivalent to adding a high-IQ co-driver to the driving process.

**Experimental Results**

DriveVLM was evaluated on the SUP-AD dataset built by the authors and the public nuScenes dataset. The results prove that the integration of large models significantly improves driving scene understanding and planning capabilities.

In the SUP-AD dataset, the evaluation metrics include scene description accuracy and meta-action planning quality (). DriveVLM uses Tencent Qwen-VL as the base model and has been fine-tuned (co-tuning) on multiple datasets including SUP-AD. Compared with several baseline large models, DriveVLM achieved the highest scores in both scene description and meta-action aspects. For example, DriveVLM's description score for scenes in the test set is 0.71, which is much higher than 0.38 for direct inference with GPT-4V, and 0.46 for other open-source models such as Lynx after fine-tuning. In meta-action sequence prediction, DriveVLM scored 0.37, also leading GPT-4V's 0.19 and CogVLM's 0.22. These results indicate that large models fine-tuned in the driving domain can more accurately describe scene key points and propose reasonable driving action sequences than general large models (GPT-4V) (). It is worth mentioning that the authors emphasize that through collaborative fine-tuning on multiple related datasets, DriveVLM not only ensures the performance of specific tasks, but also tries to maintain the original generalization ability of LLM as much as possible, without excessive overfitting.

In the planning task evaluation on the nuScenes public dataset, the authors compared the trajectory error and collision rate of DriveVLM, DriveVLM-Dual, and various existing planning methods. The results show that DriveVLM, after being combined with classic modules, can reach the current state-of-the-art level. For example, at 3 seconds of prediction, DriveVLM-Dual's average L2 error is only 0.48 m, and the average collision rate is 0.10%, which is comprehensively better than the previous best method VAD-Base (0.60 m, 0.14%). In contrast, purely visual end-to-end models such as ST-P3 have errors as high as 2.11 m and collision rates of 0.71%, and traditional multi-task joint models UniAD have errors of 1.03 m and collision rates of 0.31%. Even without combining traditional perception, the DriveVLM pure model itself also achieves an average error of 0.40 m and a collision rate of 0.27% under the condition of having map and detection input, which is comparable to the strongly supervised method VAD. It should be pointed out that the above DriveVLM results are obtained under the condition of using high-definition maps and detection and other auxiliary information (methods VAD-Base, DriveVLM, etc. in the table all use maps and detection as input), so their values are much better than end-to-end methods such as VLM-E2E or Doe-1 that do not use maps. But this precisely reflects the advantage of DriveVLM-Dual's design: while maintaining the strengths of traditional modules, VLM is introduced to improve decision-making intelligence. Finally, the authors deployed DriveVLM-Dual on actual cars for road tests and found that the vehicles can successfully cope with various complex road conditions, verifying its effectiveness. For example, in real-world tests, when encountering situations such as pedestrians suddenly crossing the road or construction occupying the road, the DriveVLM module can output decision suggestions such as "decelerate and avoid pedestrians" or "change lanes to bypass construction," and the vehicle takes safe actions accordingly. These positive results in real-world environments make DriveVLM one of the few large model autonomous driving solutions verified by real vehicles.

**Advantages and Limitations**

The advantage of DriveVLM is that it introduces powerful common sense and reasoning capabilities, significantly expanding the cognitive level of autonomous driving systems. Through VLM, vehicles no longer just react based on geometric and physical rules, but can "understand" scenes: for example, understand the intention of the vehicle in front, and understand the implicit rules in complex traffic scenes. This understanding ability is particularly valuable in long-tail scenarios—extreme situations that are rarely involved in conventional training data. Large models may supplement reasoning through common sense in their training corpus. For example, when encountering road water, a large model may infer that the vehicle should decelerate to prevent slipping, even if images of exactly the same situation have not appeared in the training set. In terms of planning, the introduction of hierarchical decision-making makes planning more transparent and controllable: meta-action sequences are similar to human drivers' decision-making steps, giving the system a certain degree of interpretability (each step has semantic meaning). Compared to end-to-end networks directly outputting steering angles, this method is convenient for human review and intervention, improving safety and regulatory friendliness. The DriveVLM-Dual architecture takes into account real-world usability: it does not completely rely on computationally expensive LLMs for real-time output, but combines lightweight traditional modules to ensure that the system can run and maintain performance at the current hardware level. Its successful deployment on real vehicles proves the effectiveness of this compromise solution—the large model module acts as an auxiliary decision-maker, improving overall performance without causing the system to lose real-time performance and stability.

The limitations of DriveVLM also partly stem from its compromise strategy and the large models it relies on: First, the Dual architecture still needs high-precision traditional perception modules and high-definition map support. This means that it is more dependent on external factors, and the system's advantages may be limited in situations where maps are missing or sensors are obstructed (such as heavy snow or heavy fog). In addition, the computational cost of large models cannot be ignored. Although the authors used the more optimized Qwen-VL and cropped some visual Adapters, the entire model still has nearly 1 billion parameters. Frequent calls during inference result in high latency and resource occupation. In real vehicles, GPU servers may be needed for auxiliary computing, and it is difficult to directly embed such a huge model in low-cost mass-produced cars in the short term. However, the Dual mode may alleviate this by reducing the LLM call frequency, for example, only calling when high-level decisions are needed, while daily cruising mainly relies on conventional planning. Second, the reliability of LLMs is still a hidden worry: the model may generate incorrect judgments or unreasonable suggestions (although fine-tuning reduces such situations). The authors have partially solved this problem by introducing traditional modules as "double insurance," but it is still necessary to prevent deviations in the output of large models. For example, if the model incorrectly ignores a danger in a visual corner, traditional perception can at least detect the object and remind the system to pay attention. This is the safety net of Dual. It is riskier if you rely entirely on large models. Therefore, the current DriveVLM choice is not completely end-to-end, but retains human-designed modules. Finally, DriveVLM uses a pre-defined set of meta-actions, which undoubtedly improves learning efficiency and interpretability, but also introduces human prior limitations. 17 types of meta-actions may not be sufficient to cover all the nuances of driving decisions (such as how to simultaneously consider longitudinal and lateral action combinations), and overly coarse-grained meta-actions may also limit the strategy space in some complex situations. However, this problem can be improved by expanding the action library or allowing LLMs to describe decisions in more free text (for example, Senna's work allows LLMs to output complete natural language decisions). Overall, DriveVLM achieves a balance between performance and practicality under current technical conditions through a hybrid architecture, but to move towards fully autonomous driving driven by large models, further efforts are needed in model efficiency and reliability.

## Comprehensive Comparison and Discussion

The three studies each introduced vision-language large models and world model ideas to improve autonomous driving, but their focuses and implementation methods are different, and each has its own advantages and disadvantages. Below, we compare them in terms of architecture integration methods, innovative highlights, experimental performance, and limitations.

### Architecture Integration Methods

VLM-E2E belongs to an auxiliary enhancement architecture. It does not change the basic structure of the end-to-end planning network, but enriches visual BEV features during the training process through a text channel generated by a pre-trained VLM. In other words, VLM itself does not participate in online decision-making, but only provides training supervision signals, and the final inference decision is still executed by a monolithic end-to-end network. This method has tight integration and low overhead, but the role of large models is limited to being a "teacher" or "feature provider." In contrast, DriveVLM is an embedded architecture, directly embedding a fine-tuned VLM module into the decision-making chain, allowing it to serve as an engine for scene understanding and high-level planning. It needs to run large models during inference and combine them with traditional modules for decision-making, and can output human-readable decision-making basis in real time (descriptions, meta-actions). Doe-1 goes to another extreme—a unified generative architecture, transforming the entire perception-control loop into a large autoregressive model. Under this architecture, there are no explicit modules, and the language ability of VLM and the generation ability of the world model are integrated, and the understanding and simulation of the environment are completed simultaneously in sequence generation. Simply put, VLM-E2E is "end-to-end model + offline VLM supervision," DriveVLM is "traditional stack + online VLM assistant," and Doe-1 is "end-to-end large model (with built-in language and visual generation capabilities)."

This difference brings about different system complexity and flexibility. The integration method of VLM-E2E has the least modification to existing end-to-end methods and is easy to implement on existing frameworks, only requiring the addition of a text branch for training. However, its semantic information is embedded in the model weights, and it lacks a flexible language interface during inference. In contrast, DriveVLM retains an explicit language interface through modular design: it can output scene descriptions, accept inquiries, and even converse with people to explain decisions. This adds interaction and interpretation dimensions to autonomous driving systems (for example, DriveVLM can explain to engineers "why I did this"). Doe-1 is more like training an intelligent agent. Its output includes both actions and descriptions, and it is naturally interpretable (the model always generates a description of the current scene first, and then gives actions), and can accept different Prompts to complete different tasks. However, Doe-1's language ability completely depends on the training corpus (such as QA pairs generated by GPT-4), which is far less powerful than the pre-trained LLM used by DriveVLM. This means that DriveVLM has greater potential in terms of common sense reasoning and free language, while Doe-1 may be limited by training data and have insufficient language diversity and depth, and the richness and accuracy of its answers may not be as good as DriveVLM in open-domain questions. For example, if you ask the system "What might be the reason why the vehicle in front suddenly stopped?", DriveVLM based on LLM may reason out various possibilities (avoiding pedestrians, engine failure, etc.) and explain them in natural language; while Doe-1 has only learned limited patterns in the training set and may not be able to give explanations beyond the scope directly related to driving.

### Comparison of Innovative Highlights

The three all have innovations in their respective works: the highlight of VLM-E2E is the proposal to explicitly integrate driver attention into end-to-end training. It solves the pain point of traditional E2E models lacking consideration of traffic rules and key semantics, and guides the model to pay attention to red lights, pedestrians, etc. through text supervision. In particular, the introduction of dynamic modal weights and strategies for balancing visual/text information has general significance in the field of multimodal fusion, which can avoid one modal information from overwhelming another modal and improve the fusion effect. The biggest innovation of Doe-1 is the idea of a closed-loop world model: it successfully unified the generation of language, vision, and control in a unified framework. This idea of equating the decision-making problem to a sequence generation problem is the same as the paradigm of the GPT series in NLP. The Doe-1 model can be regarded as "GPT in the driving world." Its breakthrough lies in proving that this paradigm is feasible for autonomous driving spatiotemporal decision-making problems, and showing the potential to perform perception, question answering, and planning multi-tasks. DriveVLM's innovation focuses on system integration and practicality: it first creatively integrated the reasoning ability of LLM and traditional driving modules, and proposed the intermediate representation of "meta-action + decision description" to bridge language decisions and numerical control () (). In addition, the SUP-AD dataset and evaluation method created by the DriveVLM team, as well as the verification on real vehicles, are also a pragmatic innovation—they not only propose methods, but also build a complete chain from data to evaluation, promoting the standardization of research in this direction.

In terms of task scope, both Doe-1 and DriveVLM pursue multi-task universality: the former schedules tasks through Prompts, one model for multiple uses, and the latter covers all links from perception to planning by designing multiple outputs (description, analysis, meta-actions, trajectories) (). VLM-E2E is relatively singular, mainly focusing on improving planning performance, without involving actively generating descriptions or answering questions. This means that in terms of explainable AI, DriveVLM and Doe-1 are one step further—DriveVLM can output human-readable decision-making basis, and Doe-1 has built-in description generation, which can answer environment-related questions. Although VLM-E2E integrates text features, these features are only embedded inside the model and will not be output to users in text form, so it improves the model's implicit cognition, not direct interpretability.

### Comparison of Experimental Performance

In terms of planning and decision-making performance, if we directly compare numerical values, DriveVLM-Dual obviously achieved the best results (3-second error 0.48 m, collision rate 0.10% on nuScenes ()). However, the fairness of comparison needs to be noted: DriveVLM-Dual has innate advantages by using high-definition maps and high-precision detection. In contrast, VLM-E2E and Doe-1 both assume that the input is only camera images (and limited high-level navigation instructions), and do not rely on additional map information. Therefore, a more reasonable comparison is to look at the improvement of each relative to the baseline under the same conditions: VLM-E2E achieves a reduction of about 5% in long-distance error (2.80 to 2.68) and a reduction of more than 50% in collision rate (2.34% to 1.17%) relative to the purely visual end-to-end baseline. Doe-1 reduces the error by about 65% (to 1.07 m, under closed-loop evaluation) compared to other methods using only front-view cameras (such as OccNet 3-second error 2.99 m). DriveVLM then continues to reduce the error by another 16% (0.37 to 0.31) relative to its baseline VAD-Base (which is already very powerful) () (). Therefore, looking at the relative improvement, VLM-E2E and Doe-1 have larger improvements on weak baselines, and DriveVLM also has improvements on strong baselines, but the magnitude is limited. However, in terms of absolute indicators, classic methods combined with large models (DriveVLM-Dual) do have the best effect, indicating that the information of traditional geometric modules (such as high-precision maps, dynamic trajectories) is significantly helpful for accuracy. This also implies an important conclusion: relying solely on vision and language, current large model methods are still difficult to completely surpass traditional methods that integrate multiple sensors, but they have shown a trend of catching up. Especially Doe-1, which almost reaches the level of multi-sensor methods with only monocular vision. If DriveVLM is stripped of map and other information, its DriveVLM pure model results (3-second error 0.68 m) are still better than Doe-1's (1.07 m). This may be attributed to DriveVLM's use of the powerful prior knowledge of pre-trained LLMs and the use of surround-view multi-camera images, while Doe-1 lacks information in this regard. However, Doe-1 excels in unified learning, and its performance has the potential to continue to improve as the model and data scale expand.

In terms of perception and semantics, VLM-E2E proves the improvement of model perception accuracy after semantic fusion through auxiliary task evaluation (BEV segmentation, prediction). DriveVLM and Doe-1 do not directly perform segmentation or detection evaluation, but indirectly measure the perception effect through question answering and description tasks. Doe-1's scene description CIDEr score exceeds OmniDrive-3D, indicating that the key information perceived by it is more comprehensive. DriveVLM's scene description score is also very high in SUP-AD (0.71 vs next best 0.49) (). If we consider interpretable perception, both DriveVLM and Doe-1 can output natural language descriptions, which VLM-E2E cannot directly do. DriveVLM can even generate verbal explanations in the car. Therefore, in terms of human-computer interaction friendliness, DriveVLM/Doe-1 > VLM-E2E.

### Comparison of Limitations

The limitation of VLM-E2E is that its applicable scope is limited: it mainly serves to improve planning performance and does not involve other uses such as explanation and simulation. At the same time, it needs labeled data to refine the output of BLIP-2, and it is difficult to directly apply it in domain-shifting or unsupervised scenarios. In addition, although its improvement is obvious, it is still not enough to cope with some extreme situations, such as when vision is severely damaged (heavy fog, strong backlight), relying solely on learned semantic attention may not be enough. In contrast, DriveVLM retains traditional perception, so it can still rely on radar, lidar, etc. in bad weather, which is more robust in this regard. However, the problem with DriveVLM is that it is more complex: it needs to train and maintain a large model module and a traditional module, and system debugging is difficult. Doe-1 takes the extreme end-to-end route. If it can be fully trained in the future, it may simplify the system, but the training cost and risk are highest at this stage. Doe-1 may also face the problem of long-term prediction drift, that is, when generating long sequences of more than 10 seconds, small errors accumulate into large deviations, which is not fully resolved in the paper. Therefore, in closed-loop simulation, it may need to be periodically reset with sensor ground truth, and it is not yet possible to rollout indefinitely. DriveVLM avoids this problem because real sensors correct the world state every moment.

A common challenge worth noting is: the safety and verification of large models. Whether it is an integrated or generative model, the introduction of learned large models brings uncertainty and verification difficulty that traditional rule systems do not have. Although VLM-E2E only uses VLM in training, the internal mechanism of the fused model is more difficult to explain, and it is not clear how to prove that it is reliable in all situations. DriveVLM and Doe-1 allow the model to output human-readable information, which alleviates the black box problem to a certain extent, but it is still necessary to establish a complete verification process to ensure that the model will not produce erroneous suggestions or predictions at critical moments. In the safety-critical field of autonomous driving, the requirements in this regard will be higher than general AI applications, requiring more rigorous testing and perhaps new verification theories.

### Comparison with Existing Literature

In terms of combining large models with autonomous driving, in addition to DriveVLM, there are many recent explorations, such as OmniDrive, GPT-Driver, DriveGPT, etc. Compared to these, VLM-E2E, Doe-1, and DriveVLM each have their own unique features: DriveVLM's real vehicle verification and performance improvement make it one step ahead in practicality, while OmniDrive, etc., focus on framework proposal and may not have actual deployment. According to reports, OmniDrive also tried a scheme of using 3D Q-Former to connect multimodal input to LLM and output driving instructions. Compared with it, DriveVLM introduced Dual hybrid, making it closer to application and achieving better results. Compared with DriveVLM, Doe-1 represents two concepts: one is integrated large model learning (closer to academic pursuit of AGI), and the other is hybrid intelligent agent combined with engineering constraints (more pragmatic and efficient). It is foreseeable that these two routes may learn from and integrate with each other in the future. For example, Doe-1's generative world model can be used for DriveVLM's scene simulation and prediction modules; DriveVLM's LLM common sense can be used to guide Doe-1's description generation or verify its predictions. In fact, there are already works in the industry trying to combine LLMs and world models, such as DriveDreamer-2 using LLMs to enhance driving video generation models to generate more diverse driving scenarios. Compared with these early attempts (mainly verified in simulation), Doe-1 and DriveVLM have pushed this idea to more complete closed-loop and real-world verification, showing leading results.

In summary, the three papers have each taken a step forward in improving the intelligence of autonomous driving: VLM-E2E proves that "supplementing semantics" to end-to-end driving models can significantly optimize decision-making quality; Doe-1 pioneered a new paradigm of using large generative models to solve driving tasks in a unified way, showing the possibility of multi-task integration; DriveVLM shows that pre-trained large models can be organically combined with existing systems to immediately improve the ability to handle complex scenarios, and has real-world feasibility. These works are more radical in architecture than each other, but they are all moving in the direction of making autonomous driving systems smarter, safer, and more interpretable.

## Outlook: Impact on the Next 3-5 Years

These explorations of integrating VLMs and world models foreshadow new trends in the development of autonomous driving technology: the future autonomous driving "brain" will no longer rely solely on rules and small neural networks, but will integrate large-scale pre-trained knowledge, modal interaction, and environmental simulation capabilities. Specifically:

- Stronger Understanding of Complex Scenarios: In the next 3-5 years, autonomous driving systems will benefit from the knowledge generalization ability of large models when dealing with rare scenarios such as construction zones, accident scenes, and special weather. DriveVLM has shown that large models can understand subtle human behaviors and complex scene contexts. As these models have larger parameters and more training data, their "knowledge" of long-tail scenarios will be richer, and reasoning will be more reliable (such as Qwen2.5-VL and Phi-4-multimodel). It may even extend Reasoning models to multimodal, becoming visual reasoning models (such as QvQ), further enhancing the model's thinking ability. For example, a trained large model may be able to correctly interpret the meaning from traffic police gestures and unconventional signs, which is difficult for traditional methods to cover. Human driving relies on a lot of experience and common sense, and large models are providing similar experience libraries. In the next few years, we may see driving AIs with common sense reasoning ability appear: it knows to be extra careful in school zones, and seeing the vehicle in front turn on hazard lights may indicate a malfunction, etc. These can all be obtained through VLM/LLM common sense learning. This will improve the robustness of autonomous driving in unfamiliar cities and extreme conditions.
- Decision Interpretability and Human-Computer Interaction: Since autonomous driving involves safety, the transparency of its decisions is very important. Integrating language models enables the system to explain itself in a way close to natural language. Future autonomous vehicles may be able to describe the environment and their intentions to passengers in real time. For example: "I am decelerating because pedestrians have been detected at the crosswalk"—this type of explanation can enhance passengers' trust in AI and facilitate developers' debugging. DriveVLM has already made attempts in this regard, and the Dual system can output decision descriptions (). Within 3-5 years, with regulatory requirements and user expectations, explainable AI driving assistants may become standard. Passengers can ask the vehicle: "Why did you suddenly brake just now?", and the system will answer through the VLM module: "The vehicle in front was detected to have braked suddenly, and I braked to maintain a safe distance." Similar scenarios will improve user experience and safety confidence.
- Closed-Loop World Models and Simulation Training: The world model demonstrated by Doe-1 can generate future scenes based on actions, and this capability has a wide range of uses in future development. First, it can be used to build closed-loop simulation environments: AI can simulate the consequences of different decisions in its mind, and then choose the optimal solution. This is similar to human drivers mentally simulating "what will happen if I change lanes now." Future driving algorithms may quickly rollout several solutions internally before each execution, predict collision risks and traffic efficiency, and then make a decision (requires extremely high computational efficiency, but it is expected to be achieved). Second, world models can also be used for data augmentation and simulation training. Autonomous driving real data collection is expensive, and a powerful generative world model can simulate realistic driving videos and events, providing "infinite" samples for training. Large models like DriveVLM that require a large amount of training data can use this generated data to make up for long-tail distributions and improve model performance. Especially in safety testing, realistic simulation can cover extreme situations, and the scenarios generated by large models may be richer and more varied than existing game engines because it can learn based on real data. In the next few years, we may see large model-driven autonomous driving simulators appear, used to verify algorithms and train decision-making networks, and their scene diversity and realism far exceed existing simulations.
- Hybrid Planning Paradigm: The hybrid planning idea led by DriveVLM may become mainstream. That is, high-level decisions are provided by large models with flexibility and intelligence, and low-level control is guaranteed by reliable algorithms. Such a two-layer structure is in line with the engineering path of gradually increasing trust. In 3-5 years, it is still difficult to completely use end-to-end large models to take over driving (mainly reliability and regulatory issues), so the architecture of human guidance + AI decision-making + rule bottom-line will be an ideal solution for the transition period. For example, a vehicle's advanced AI is responsible for understanding complex scenes and formulating tactics (how to change lanes and overtake), and then handing over the targets to low-level control for precise implementation. This is similar to the relationship between flight management systems and autopilots in the aviation field. The hybrid paradigm can also easily incorporate human intervention: in a few extreme situations, human remote monitoring can directly intervene or guide large model decisions through brief instructions to ensure safety.
- Model Efficiency and Customization: Currently, these large models all face the problem of high computational load, so one of the future research focuses is model compression and dedicated chips. In 3-5 years, medium-sized vision-language models optimized for autonomous driving (with moderate parameters but retaining key common sense abilities) may appear, or large model knowledge may be distilled into small models for deployment through distillation techniques. VLM-E2E actually provides a distillation idea: using attention cues generated by large models to train a small model. In the future, a wider range of large model knowledge (not only attention, but also traffic rule common sense, etc.) may be used to supervise small model training. This can both enjoy the benefits of large models and ensure real-time performance. In terms of hardware, the autonomous driving domain has high demand for computing power, which may promote the emergence of in-vehicle AI acceleration chips, specifically accelerating the inference of models like Transformer, making it possible to run 10 billion parameter models on the vehicle end. Within 3-5 years, high-end autonomous vehicles may be equipped with AI modules with computing power of hundreds of TOPS or even POPS level, which is sufficient to deploy a trimmed large driving model for real-time operation.
- Safety and Standards: The emergence of these works will also promote the evolution of autonomous driving AI safety standards. Regulators may require special assessments of large model-driven decisions, such as verifying that the model will not produce dangerous hallucinations. The industry may develop test question banks to assess AI drivers in a combination of language and scenarios, such as asking "What would you do if the vehicle in front suddenly changed lanes?" and checking the model's answer. This is similar to a combination of driver's written test + road test, except that the object is AI. The emergence of VLM fusion architecture just provides an interface for AI to participate in this test (AI can explain its response in natural language). Therefore, in future standards, whether autonomous driving AI can correctly answer driving-related questions and whether it can show safe driving behavior in simulated environments may become part of the certification. In short, the introduction of large models forces us to examine autonomous driving systems from a cognitive intelligence level, not just the correctness of perception and control.

Summary: The continuous progress of VLM-E2E, Doe-1, DriveVLM and other works is leading autonomous driving from the traditional paradigm of "perception - planning - control" to a new stage of "cognition - reasoning - action." In the next 3-5 years, we expect vision-language large models to be deeply integrated into autonomous driving: in the short term, it will mainly focus on auxiliary decision-making and enhanced safety (such as in the form of DriveVLM-Dual), while in the medium and long term, it is expected to develop highly autonomous driving intelligent agents. Although there are still many challenges to be solved, such as model reliability, real-time performance, and legal responsibility delineation, these studies have proven that large models have the potential to make autonomous driving vehicles smarter and safer. When vehicles can understand the surrounding world and predict the intentions of others like experienced drivers, true driverless cars will be one step closer. As implied in the title of the Doe-1 paper: we are moving towards the era of large driving world models, and in this process, the fusion of VLMs and world models will play a key role, injecting unprecedented vitality and wisdom into autonomous driving technology.

## References

- [VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion](https://arxiv.org/html/2502.18042)
- [Doe-1: Closed-Loop Autonomous Driving with Large World Model](https://ar5iv.org/html/2412.09627)
- [DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/abs/2402.12289)
