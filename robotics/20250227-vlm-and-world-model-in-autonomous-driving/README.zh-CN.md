# 智驾新引擎：VLM和世界模型如何重塑自动驾驶技术

by @corenel (Yusu Pan) and OpenAI Deep Research

## 目录

- [智驾新引擎：VLM和世界模型如何重塑自动驾驶技术](#智驾新引擎vlm和世界模型如何重塑自动驾驶技术)
  - [目录](#目录)
  - [引言](#引言)
  - [论文分析](#论文分析)
    - [VLM-E2E：多模态驾驶员注意力融合的端到端驾驶](#vlm-e2e多模态驾驶员注意力融合的端到端驾驶)
    - [Doe-1：大型世界模型实现闭环自动驾驶](#doe-1大型世界模型实现闭环自动驾驶)
    - [DriveVLM：大规模视觉语言模型与自动驾驶收敛](#drivevlm大规模视觉语言模型与自动驾驶收敛)
  - [综合比较与讨论](#综合比较与讨论)
    - [架构融合方式](#架构融合方式)
    - [创新亮点比较](#创新亮点比较)
    - [实验性能比较](#实验性能比较)
    - [局限性比较](#局限性比较)
    - [与既有文献的对比](#与既有文献的对比)
  - [展望：对未来 3-5 年的影响](#展望对未来-3-5-年的影响)
  - [参考材料](#参考材料)

## 引言

自动驾驶在复杂多变的交通环境中面临诸多挑战，如长尾场景（罕见或复杂情况）的理解以及高效决策。近期的研究趋势是将视觉 - 语言大模型（Vision-Language Models, VLM）以及世界模型引入自动驾驶领域，通过融合多模态感知与生成式预测来增强车辆的环境理解和规划能力。这类融合架构旨在让自动驾驶系统具有类似人类驾驶员的高层语义推理和注意力机制，同时能够模拟和预测环境变化，实现闭环决策。本文将深入分析三篇代表性论文在“VLM/VLA+ 世界模型”融合架构中的应用：① VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion，② Doe-1: Closed-Loop Autonomous Driving with Large World Model，③ DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models。我们将分别阐述每篇工作的具体贡献、方法创新与实验结果，并比较它们在融合 VLM 与世界模型架构上的异同，评估其优势与局限。此外，我们将讨论这些研究对未来 3-5 年自动驾驶技术发展的潜在影响，并将它们的成果与既有文献（如 DriveVLM 等）进行对比评述。

## 论文分析

### VLM-E2E：多模态驾驶员注意力融合的端到端驾驶

**具体贡献**

VLM-E2E 提出了一种利用视觉 - 语言模型增强端到端自动驾驶的方法，引入多模态驾驶员注意力来指导决策。其核心贡献在于：

1. 将预训练的视觉 - 语言模型提供的语义描述作为辅助信息融入端到端驾驶模型中，显式注入人类驾驶员的注意力语义；
2. 设计了 BEV- 文本特征的可学习加权融合策略，动态平衡视觉 BEV 特征与文本特征的重要性；
3. 提出利用文本语义精炼技术缓解大模型幻觉问题，确保引入的描述准确且与驾驶任务相关。通过这些创新，VLM-E2E 使端到端模型能够学习更丰富的特征表示，对关键交通语义有更接近人类的关注。

**方法与创新点**

传统端到端驾驶多从多摄像头图像提取 Bird’s-Eye-View (BEV) 特征，再预测未来轨迹或控制指令。然而，此过程常在 2D 到 3D 转换时丢失高层语义信息，难以模拟人类注意力。VLM-E2E 利用预训练视觉 - 语言模型 BLIP-2 从前视摄像头画面生成场景的初始文本描述，着重提及驾驶员关注的关键对象和事件（如行人、红绿灯等）。为避免大模型可能出现的幻觉（例如描述中出现实际不存在的元素），作者使用场景真值标注和高层驾驶意图对生成文本进行精炼校正，使描述既准确又契合驾驶任务。精炼后的文本再通过预训练的 CLIP 模型编码为文本特征，与视觉 BEV 特征对齐到同一嵌入空间。在融合阶段，引入一个可学习权重来动态调整 BEV 特征和文本特征的占比。这种机制保证了模型既不会过度依赖视觉几何信息也不会忽视语言语义信息，从而获取更加全面稳健的环境表示。值得注意的是，作者还针对驾驶任务设计了特定的提示词 (prompt) 输入 BLIP-2，以提取“可行动的注意力信息”，过滤无关细节，强化与驾驶决策相关的要素。总体而言，VLM-E2E 的创新在于首次将预训练 VLM 生成的驾驶语义提示融入端到端规划模型，赋予模型高层语义推理能力的同时保持几何感知精度。相比之前仅用 VLM 生成轨迹再精修的方案（如 DriveVLM 将 VLM 输出轨迹供 E2E 模型细化），VLM-E2E 以更紧密的特征级融合实现了真正多模态联合训练。

**实验结果**

在 nuScenes 数据集上，VLM-E2E 通过消融和对比实验展示了显著优于现有方法的性能提升。其改进体现在自动驾驶的三个关键子任务：感知、预测和规划。在感知评估上，模型输出的 BEV 语义分割在可驶区域、车道线、车辆、行人四类中有三类 IoU 达到当前最佳；例如，与经典 BEV 分割方法相比，车辆和可驶区域的 IoU 分别提升至 28.17→更高、65.97→更高（具体数值作者报告为在 3/4 类别上取得最佳）。在未来帧预测（2 秒后场景预测）上，VLM-E2E 的语义和实例分割精度同样领先：对比 FIERY 等方法，IoU 提高约 1.6 个百分点（36.89→38.54），PQ 提高近 0.7。规划任务的结果尤为突出：在长期轨迹精度和安全性上都有大幅提升。以 3 秒预测为例，VLM-E2E 的轨迹 L2 误差为 2.68 m，优于所有对比方法（相比传统“Vanilla”基线的 2.80 m 有所降低）。尤其值得关注的是碰撞率（Collision Rate）指标：在 2 秒和 3 秒预测范围内，VLM-E2E 的碰撞率分别降至 0.60% 和 1.17%，为所有方法中最低。相比之下，次优方法 ST-P3 在 2 秒时为 0.62%，3 秒时为 1.27%，而其他方法如 Vanilla 的碰撞率高达 2% 以上。这表明 VLM-E2E 在较长时间尺度上的规划鲁棒性和安全性显著增强。例如，虽然在 1 秒短预测下某些基线略占优（ST-P3 在 1 秒碰撞率 0.23% 略低于 VLM-E2E 的 0.26%），但 VLM-E2E 在 2-3 秒区间持续保持优势，能更好地避免潜在碰撞。作者的消融实验还验证了各模块作用：移除文本融合或不进行权重平衡都会使性能下降，证明语义注意力信号和动态融合对提升效果是不可或缺的。总体而言，VLM-E2E 以显著更低的长距离误差和碰撞风险，优于过去的端到端方法，展示了多模态注意力融合在复杂场景驾驶决策中的价值。

**优势与局限**

VLM-E2E 的优势在于提高了端到端驾驶模型对高层语义和关键注意力的感知。通过模仿人类驾驶员关注交通标志、行人动态等过程，模型决策更符合人类直觉，从而在复杂场景下表现更优。如结果所示，特别是在夜间行人、繁忙交叉口等需要语义理解的情境下，模型能更好地识别风险并规划反应，这对提高自动驾驶安全性和解释性都有帮助。此外，VLM-E2E 仅在训练阶段引入 VLM 生成的文本作为辅助，并不需要在推理时实时调用大模型，因此推理开销较小。这意味着在部署时无需额外的巨型模型计算，大幅降低了实际应用门槛。

然而，其局限也值得注意：首先，该方法依赖已有标注数据和预训练模型进行语义精炼，需要高质量的标注（如准确的 3D 检测和高层意图）来指导文本修正。这在数据匮乏或标注不准的情况下可能效果受限。其次，引入的文本描述质量对训练效果很关键；尽管通过真值校正减少了幻觉，仍可能存在描述不完备的情况（例如模型只关注了行人但漏描述了车辆）。另外，VLM-E2E 目前仍属于开环规划评估，即根据当前观察直接输出轨迹，并未将预测结果反馈再循环利用。这意味着它尚未完全解决决策链条中的闭环问题，模型只能做到“更智能地输出下一步”，但没有内置机制验证或多步推演其决策后果。这一点在后续的 Doe-1 工作中得到了进一步探索。最后，相较一些直接利用 LLM 推理的方案，VLM-E2E 的语义信息是“静态融合”进模型权重内的，推理时虽然高效，但灵活性略逊——若遇到训练集中未见过的新场景，其语义理解能力取决于模型已有的表征，一定程度上受训练分布限制。

### Doe-1：大型世界模型实现闭环自动驾驶

**具体贡献**

Doe-1 提出了首个将生成式大型世界模型用于闭环端到端自动驾驶的框架，实现了感知、预测、规划三合一的统一模型。主要贡献在于：

1. 将自动驾驶过程形式化为“下一个 Token 生成”问题，利用多模态离散 Token 序列同时表征场景描述、未来预测和驾驶动作。通过这种表示，感知、规划、预测可以在同一 Transformer 模型中以自回归方式连续生成，形成闭环；
2. 设计了针对驾驶的 Token 化方案：采用自由文本描述作为感知输出 Token（刻画当前观察的语义场景）、用图像 Token 直接生成下一时刻的图像预测、以及使用位置敏感的离散动作 Token 编码车辆控制指令。这种方案巧妙地将连续的感知和控制信号离散化，便于 Transformer 处理；
3. 训练了一个多模态 Transformer 模型（Doe-1），能够自回归地生成“描述 - 动作 - 下一观察”循环。该模型在 nuScenes 数据集上被验证可用于多种任务，包括视觉问答（评估感知语义）、动作条件的视频生成（评估预测能力）和运动规划（评估决策质量）。

换言之，Doe-1 开创性地将大模型用于生成闭环驾驶序列，在单一模型中统一了传统上分离的感知、预测、规划模块。这是对现有大模型用于驾驶方法的一次重大跃升：此前的一些尝试或直接调用 LLM 做问答/决策，或引入 VLM 辅助但仍非闭环；Doe-1 则真正让模型学会“从观察生成描述，再决定动作，再预见下一观察”的全过程，使模型本身成为一个驾驶代理（driving agent），具备一定的环境内生模拟能力。

**方法与创新**

在方法上，Doe-1 的关键在于设计统一的序列表示和闭环生成机制。首先，对于每一时间步，环境由六路围绕相机观测（nuScenes 提供）和相应高层语义描述来表征。作者使用自然语言完整描述当前场景（包括动态对象、道路环境等），将其编码为一串文本 Token。接着，以这些描述为条件，模型生成对应的驾驶动作 Token，其动作定义为未来短时间内的车辆位移（∆x, ∆y) 和航向变化 (∆yaw) 等，在鸟瞰图坐标系下量化成离散符号，并通过作者提出的位置编码策略确保模型感知到动作的空间意义。最后，给定动作，模型会生成下一时刻的观察图像 Token 序列，即直接在图像像素空间进行预测。训练时，将上述“观察描述→动作→下一观察”的三元组视作连续序列，让 Transformer 学习在一个循环中依次输出描述、动作、下一帧，从而在时序上闭合决策环。这种闭环自回归生成意味着模型不仅学习了感知（Observation→Description），也学习了决策（Description→Action）和状态转移（Action→Observation）。推理时，模型可以从初始观察出发，反复生成描述 - 动作 - 新观察，如同人类司机不断观察环境、解读后采取行动，再观察新环境的过程。因此，Doe-1 本质上是一个多模态序列生成模型，其“世界模型”体现在能够根据动作想象出未来的视觉场景。

Doe-1 的创新点包括：

1. 统一的多模态 Token 表示：此前工作通常将感知、规划拆分处理，或仅用文本输出高层决策。Doe-1 通过文本和图像 Token 串联，能够在同一模型中学习高层语义与低层像素的关联，这是首创的尝试。
2. 闭环训练范式：与传统开环方法（模型只输出下一步，不模拟后续影响）不同，Doe-1 让模型直接预测自己动作的后果（下一帧图像），等于内置一个环境模拟器，从而在训练中就构建了决策闭环。这种思路有利于模型学习高阶因果关系（例如连续两个动作可能导致的累积效果）和交互反馈。
3. 多任务一致性：作者巧妙地将视觉问答、视频生成、轨迹规划等任务都表述为对 Doe-1 的不同 Prompt 序列设计，使单一模型在不同任务上迁移，无需为每个任务训练专门模型。例如：提供仅当前观察作为输入 Prompt 则生成场景描述和回答问题，用当前观察 + 给定未来动作序列作为 Prompt 则生成未来图像序列，用过去观测 + 历史运动作为 Prompt 则生成未来轨迹。这种灵活的任务定义展示了大型世界模型的通用性。
4. 动作离散编码与尺度处理：采用格网量化的 BEV 坐标离散动作并加上位置编码，使得连续控制问题转化为可预测的有限词表序列。这一技术难点的解决使 Transformer 可以胜任轨迹输出，相比直接输出连续值大幅降低了难度。

**实验结果**

作者在 nuScenes 数据集上对 Doe-1 进行了全面评估，包括视觉问答（感知与语义理解)、动作条件视频生成（预测）、端到端规划（决策）三方面。结果表明，尽管 Doe-1 仅使用前视单目相机画面作为输入且训练监督主要来自高层描述和 QA，模型依然展现了有竞争力的性能。

在视觉问答任务上，Doe-1 使用 OmniDrive 提供的 nuScenes 问答基准（通过 GPT-4 生成的问题和答案）进行评测。该任务包括两类：一是场景描述的字幕生成质量，二是针对假设条件的反事实推理准确度。评测指标采用了语言生成常用的 METEOR、CIDEr、ROUGE 衡量描述与答案的文本相似度，以及 AP/AR 评估模型对若干关键事件预测的准确率和召回率。结果显示，Doe-1 的表现接近甚至超越了此前 OmniDrive 方法：例如，在描述质量上，Doe-1 的 CIDEr 得分为 72.8，明显高于 OmniDrive-3D 的 68.6；ROUGE 得分 35.6 也领先于后者的 32.6。对于反事实问题（如给定一条假想轨迹会发生什么），Doe-1 预测碰撞、越界等事件的平均精度 AP 达到 54.5%，相较 OmniDrive-3D(52.3%) 略有提升，并在召回 AR 上达到了 54.0%。值得注意的是，OmniDrive-3D/2D 利用了多相机甚至 3D 点云信息，而 Doe-1 仅凭单目图像取得了相当的结果，体现了大模型强大的语义理解能力。

在动作条件的视频生成任务中，Doe-1 需要根据给定的控制动作序列，迭代地生成未来若干帧的图像，以模拟车辆执行该动作后的观测变化。评估指标采用 Fréchet Inception Distance (FID) 衡量生成视频相对于真实序列的视觉质量。对比多种现有世界模型：GAN 类的 DriveGAN、扩散模型类的 DriveDreamer、WoVoGen、Drive-WM、GenAD，以及最新的高保真生成模型 Vista，Doe-1 取得了 FID=15.9。这一成绩与 Drive-WM (15.8) 和 GenAD (15.4) 相当，明显好于早期方法如 DriveGAN (73.4) 和 DriveDreamer (52.6)。虽然未能超越目前最好的 Vista (FID=6.9) ，但考虑到 Vista 等是专门为图像生成优化的扩散模型，而 Doe-1 是在统一模型中兼顾多任务，其图像预测质量能够接近专用模型已实属难得。更重要的是，从生成的视频序列来看，Doe-1 不仅保证了图像质量，还能维持场景中 3D 结构的一致性和与给定动作的协调。作者提供的可视化显示，Doe-1 能生成与实际驾驶相符的场景变化，例如车辆转向时道路视角的转变、加速时前车距离的拉近等，证明模型确实学会了物理合理的世界模拟。

在端到端运动规划评估中，作者采用了标准的轨迹 L2 误差和碰撞率指标，在预测 1 秒、2 秒、3 秒后的位置误差及是否碰撞进行统计。Doe-1 仅利用摄像头图像和高层 QA 监督进行训练（不使用地图、检测等额外信息），其规划性能达到了与多种现有方法相当的水平。例如，与纯视觉的 ST-P3 方法相比，Doe-1 在 3 秒平均 L2 误差上更低（2.11 m vs 2.90 m），3 秒碰撞率也接近 ST-P3 (约 1.19% vs 1.27%)。不过，相较融合多模态信息的最新方法，Doe-1 在开环评估下略有差距：如最新的 GenAD 使用摄像头加地图和检测监督，3 秒 L2 仅 0.91 m，碰撞率 0.43%；而 Doe-1 相应指标为 1.26 m 和 0.53%。尽管如此，考虑到 Doe-1 没有借助激光雷达或高清地图，其取得的平均误差 1.26 m 仍属于较好范围（约一辆车身长度），平均碰撞率 0.53% 也与许多传统方法相当。作者进一步采用 VAD 论文中的闭环模拟评测方法（标注为†）对 Doe-1 进行评估，发现模型性能有明显提升：Doe-1 在闭环评测下 3 秒 L2 误差降至 1.07 m，碰撞率仅 0.47%。这一改善表明，在循环滚动模拟自身决策的过程中，模型或许能够纠正部分误差，使轨迹更接近可执行且安全的范围。不过，即便如此，Doe-1 的规划尚未超越最佳传统分模块方法。例如，结合多传感器和丰富监督的 OmniDrive 框架，3 秒 L2 仅 0.55 m，碰撞率 0.30%；另一方法 GenAD (†) 达到 0.78 m 和 0.19% 的出色水平。总体而言，Doe-1 作为单目闭环模型，在规划准确性上略逊于融合多传感器的 SOTA，但已经展示出接近人类驾驶水平的决策能力：模型常能输出平滑且遵守规则的轨迹，在大部分场景下避免碰撞发生。更难能可贵的是，它同时还能解释环境并预测后果（通过描述和生成未来画面），这是传统规划模型无法做到的。

**优势与局限**

Doe-1 的主要优势在于统一性和闭环性。一个模型即可完成感知、决策、预测三大任务，不再需要复杂的流水线集成，避免了各模块级联误差和不一致问题。这种“大一统”架构使模型对环境的理解和行动的推断共享同一套表示，因而能够学习到高阶交互关系（例如模型在学习“闯红灯”这种描述与之后发生碰撞的关联）。闭环生成让模型具备“内省”能力：它在生成规划的同时，也在模拟规划后的结果，这有助于模型选择更安全的动作（正如人类驾驶会预想“我这样做会不会出事”）。实验中问答与反事实推理的加入，则进一步提高了模型对因果关系的掌握。另外，Doe-1 的多任务能力意味着更高的数据利用率和拓展性——训练中融合了描述、QA、规划等信号，使模型拥有丰富的知识，可“一专多能”地适应新任务（如回答乘客提问、报告路况等），只需调整输入提示而无需重新训练。这预示了未来自动驾驶大模型作为通用智能体的潜力。

当然，Doe-1 也存在局限和挑战：首先，模型规模与计算是现实应用的障碍。作者使用了预训练 7 亿参数（Lumina-mGPT 7B 基础上扩展视觉部分）的模型并进行了数轮微调训练，这对算力和数据都有较高要求。推理时，自回归地生成文本和图像 Token 亦较耗时，目前尚难以在车辆上实时运行完整的生成循环。其次，传感器信息有限：Doe-1 主要利用 RGB 相机，未充分利用激光雷达、车速里程计等信息。虽然多摄像头图像可以提供周边视野，但深度和车速等信息缺失可能导致模型对距离和运动估计不如基于 LiDAR 的方法准确。第三，模型性能尚待提升：从规划结果看，Doe-1 在精度和安全性上距行业最佳仍有差距。尤其在复杂场景，如雨天夜晚、多车交互，模型可能出现描述不充分或预测偏差的情况。此外，生成模型固有的幻觉和累积误差问题依然存在：如果模型在某步生成了轻微偏差的图像或描述，后续步骤可能将误差放大。虽然作者通过训练缓解了 LLM 的幻觉，但在长时间滚动模拟时，小错误累积可能导致与真实环境渐行渐远。这限制了闭环模拟的时长，目前或许只能可靠地模拟数秒到十数秒。最后，Doe-1 作为开创性工作，为实现完整闭环提供了范式，但如何与现实结合仍需探索。其输出的图像预测目前主要用作评估或提供可视化，尚未用于控制反馈；在真实车辆中，要闭环还需将生成帧与车辆传感器对接，并处理现实中不可预知的噪声和突发情况。总的来说，Doe-1 证明了大型生成模型在自动驾驶上的巨大潜力，但在走向工程应用之前，还需在模型效率、多模态融合以及安全可靠性等方面继续改进。

### DriveVLM：大规模视觉语言模型与自动驾驶收敛

**具体贡献**

DriveVLM 工作将大型视觉语言模型直接引入自动驾驶系统，旨在通过高级语义推理增强环境理解和规划决策。其贡献体现在：

1. 提出了 DriveVLM 框架，将预训练的 Vision-Language Model (如 Qwen-VL 等) 用于自动驾驶场景的理解与决策，构建了场景描述、场景分析、分层规划三个推理模块的组合，以提升系统对复杂场景的处理能力；
2. 针对 VLM 在空间推理精度和计算负担方面的不足，引入 DriveVLM-Dual 混合系统：将 DriveVLM 的大模型推理与传统自动驾驶感知规划流水线相结合，形成优势互补的混合架构。具体来说，Dual 系统保留经典的感知模块（3D 检测、跟踪、地图等）以提供精准的低级障碍物信息，同时利用 VLM 模块进行高层语义理解和决策意图推导，从而既获得了大模型的智能推理能力，又保证了实时性和精度。
3. 作者构建了一个用于场景理解与规划的多模态数据集（SUP-AD），并设计了针对场景描述和元动作规划的评测指标，用以训练和评估 DriveVLM。
4. DriveVLM-Dual 已在实际道路测试中部署验证，成功运行于真实车辆，证明了其在现实场景的有效性和潜在可行性。总之，DriveVLM 是首批将大规模视觉语言模型闭环引入真实自动驾驶的工作之一，在方法和系统上都具有开创意义。

**方法与创新点**

DriveVLM 的方法特点是让预训练大模型参与决策过程，同时通过架构设计解决大模型直接控制车辆的难点。其工作流程通常如下：

1. 首先，摄像头等传感器数据输入传统感知模块，得到如检测的车辆、行人列表，道路拓扑等信息。然后，将这些感知结果转化为场景的抽象描述，例如以文字形式概括：“前方 100 米有施工，占用左侧两车道，右侧有行人正在过马路”等。DriveVLM 利用一个经过精调的大型视觉语言模型（如 Qwen-VL 7B 参数模型）来生成这样的场景描述和分析。具体实现上，作者将视觉特征（例如多帧图像）编码后与人为设计的提示词一起输入 VLM，让它输出关于场景的自然语言描述和场景分析。这里“场景分析”通常指对关键信息的提炼和推理，例如识别当前场景的风险点、意图解读等。
2. 接下来，DriveVLM 引入分层规划的思想：VLM 基于对场景的理解，先输出高层次的元动作 (meta-actions) 序列和决策描述，然后由传统规划模块将这些元动作细化为具体的轨迹和控制。元动作可以理解为短时间内的策略指令，例如“减速”、“向右并线”、“保持直行”等，作者预定义了 17 类常见驾驶动作以供模型选择组合。这些元动作序列比起连续的坐标更符合 LLM 的擅长模式——离散的语言形式，也更容易融入安全规则（如限定某动作序列表示保守或激进驾驶）。DriveVLM 的大模型会逐步规划：先根据场景生成一系列元动作及对应原因描述，然后传统控制模块（或一个轻量数据驱动模型）将这些转换成一系列空间坐标或路径，使车辆执行。这种两级规划相当于将复杂规划问题分解为“想做什么”（元动作序列，由 VLM 给出) 和“怎么做”（轨迹实现，由算法完成) 两个阶段。由于传统模块在第二阶段把关，LLM 即使在动作顺序上稍有不合理之处，也可被过滤或调整，从而保障最终轨迹的可行性。这实现了语义推理与精确控制的优势互补。

DriveVLM-Dual 进一步在系统架构上做创新：它并非完全用端到端学习替换传统栈，而是融合了两套系统。传统模块持续提供基础感知（确保检测精度和时效），而大模型模块作为一个“智慧大脑”提供高层决策建议。车辆最终执行的动作由两者共同决定：如果大模型的建议与传统规划矛盾，系统可以进行仲裁或选择保守方案。这样的设计考虑到当前 VLM 的计算延迟，Dual 模式可以让车辆在每帧仍用经典规划前进，而 VLM 则以较低频率提供策略指导，相当于人为在驾驶过程加入一个高智商的副驾驶。

**实验结果**

DriveVLM 在作者构建的 SUP-AD 数据集以及公开的 nuScenes 数据集上进行了评测，结果证明融合大模型显著提升了驾驶场景理解和规划能力。

在 SUP-AD 数据集中，评估指标包括场景描述准确度和元动作规划质量 ()。DriveVLM 使用了腾讯 Qwen-VL 作为基础模型，并在包括 SUP-AD 在内的多数据集上进行了精调（co-tuning）。与数个基线大模型比较，DriveVLM 在场景描述和元动作两方面均取得最高分。例如，DriveVLM 对测试集中场景的描述评分为 0.71，远高于直接用 GPT-4V 推理的 0.38，以及其他开源模型如 Lynx 微调后的 0.46。在元动作序列预测上，DriveVLM 得分 0.37，同样领先于 GPT-4V 的 0.19 和 CogVLM 的 0.22。这些结果表明，经过驾驶域精调的大模型可以比通用大模型（GPT-4V）更准确地叙述场景要点、提出合理的驾驶动作序列 ()。值得一提的是，作者强调通过在多种相关数据集上协同微调，DriveVLM 既保证了特定任务性能，又尽量保持了 LLM 原有的泛化能力，不致过度拟合。

在 nuScenes 公开数据上的规划任务评估中，作者比较了 DriveVLM、DriveVLM-Dual 与多种现有规划方法的轨迹误差和碰撞率。结果显示，DriveVLM 经过与经典模块结合，可以达到当前最先进水平。例如，在预测 3 秒处，DriveVLM-Dual 的平均 L2 误差仅 0.48 m，平均碰撞率 0.10%，全面优于之前的最佳方法 VAD-Base (0.60 m, 0.14%)。相较之下，纯视觉端到端模型如 ST-P3 误差高达 2.11 m、碰撞率 0.71%，传统多任务联合模型 UniAD 误差 1.03 m、碰撞率 0.31%。即使是不结合传统感知，单纯 DriveVLM 模型本身在有地图和检测输入的条件下也达到 0.40 m 平均误差、0.27% 碰撞率，与 VAD 这种强监督方法相当。需要指出，上述 DriveVLM 的结果是建立在使用高清地图和检测等辅助信息的条件下获得的（表中方法 VAD-Base、DriveVLM 等均利用地图和检测作为输入），因此其数值远优于 VLM-E2E 或 Doe-1 那种不使用地图的端到端方法。但这正体现了 DriveVLM-Dual 设计的优势：在保持传统模块强项的同时，引入 VLM 提高决策智能。实验最后，作者将 DriveVLM-Dual 部署到实际汽车进行道路测试，发现车辆能够成功应对多种复杂路况，验证了其有效性。例如，在现实测试中，当遇到行人突然横穿、施工占道等情况时，DriveVLM 模块可以输出“减速避让行人”或“变道绕行施工”的决策建议，车辆据此采取了安全的动作。这种真实环境下的正面结果使 DriveVLM 成为少数经实车验证的大模型自动驾驶方案之一。

**优势与局限**

DriveVLM 的优势在于引入了强大的常识与推理能力，显著拓展了自动驾驶系统的认知层次。通过 VLM，车辆不再只是基于几何和物理规则做反应，而是能够“读懂”场景：例如明白前车行为意图、理解复杂交通场景中的隐含规则等。这种理解力特别在长尾场景下有价值——常规训练数据少涉及的极端情况，大模型可能通过其训练语料中的常识来进行推理补充。例如，遇到道路积水，大模型可能推断车辆应减速防滑，即使训练集中未出现过完全相同的图像。在规划方面，引入分层决策使规划更透明和可控：元动作序列类似人类驾驶员的决策步骤，让系统具有一定可解释性（每一步都有语义含义）。与端到端网络直接输出转角相比，这种方式方便人为审查和干预，提高了安全性和监管友好度。DriveVLM-Dual 架构则兼顾了现实可用性：没有完全依赖计算昂贵的 LLM 实时输出，而是结合轻量传统模块，确保系统在当前硬件水平下可以运行并保持性能。其在实车上的成功部署证明了这种折中方案的有效——大模型模块作为辅助决策者，提升整体性能的同时未令系统失去实时性和稳定性。

DriveVLM 的局限性也部分源自其折中策略和所依赖的大模型：首先，Dual 架构中仍然需要高精度的传统感知模块和高清地图支撑。这意味着对外部依赖较强，在地图缺失或传感器受阻（如大雪、大雾）情况下，系统优势可能受限。另外，大模型的计算成本不可忽视。尽管作者采用了较优化的 Qwen-VL 并裁剪了一些视觉 Adapter，但整个模型仍近 10 亿参数。推理时若频繁调用，延时和资源占用都较高。在实车中可能需要 GPU 服务器辅助运算，短期内难以在低成本量产车上直接嵌入如此庞大的模型。不过，Dual 模式或可通过降低 LLM 调用频率来缓解，例如只在需要高层决策时调用，而日常巡航主要依靠常规规划。其次，LLM 的可靠性仍是隐忧：模型可能生成不正确的判断或不合理的建议（尽管精调减少了此类情况）。作者通过引入传统模块作为“双保险”一定程度上化解了这一问题，但仍需防范大模型输出的偏差。例如模型错误地忽略了某个视觉角落的危险，传统感知至少能检测出物体提醒系统注意，这是 Dual 的安全网。如果全盘依赖大模型则风险较高。因此当前 DriveVLM 选择并未完全端到端，而保留了人为设计模块。最后，DriveVLM 采用预定义元动作集合，这固然提升了学习效率和可解释性，但也引入了人为先验限制。17 类元动作可能不足以覆盖所有驾驶决策的细微差别（例如如何同时考虑纵横向动作组合），过于粗粒度的元动作也可能在某些复杂情况下限制策略空间。不过这个问题可以通过扩充动作库或让 LLM 以更自由的文字描述决策（比如 Senna 工作让 LLM 输出完整自然语言决策来改进。总体来看，DriveVLM 通过混合架构在当前技术条件下实现了性能与实用性的平衡，但要迈向完全依靠大模型驱动的自动驾驶，还需在模型高效化和可靠性方面进一步努力。

## 综合比较与讨论

三项研究各自引入了视觉 - 语言大模型和世界模型思想以改进自动驾驶，但侧重点和实现方式不同，互有优劣。下面我们从架构融合方式、创新亮点、实验性能和局限性等方面对它们进行比较。

### 架构融合方式

VLM-E2E 属于辅助增强型架构，它并未改变端到端规划网络的基本结构，而是在训练过程中通过一个预训练 VLM 生成的文本通道来丰富视觉 BEV 特征。换言之，VLM 本身不参与在线决策，仅提供训练监督信号，最终还是由一个单体的端到端网络执行推理决策。这种方式融合紧密、开销小，但大模型的作用有限为“教师”或“特征提供者”。相较之下，DriveVLM 是嵌入型架构，直接把一个精调后的 VLM 模块嵌入决策链，让其作为场景理解和高层规划的引擎。它需要在推理时运行大模型，结合传统模块进行决策，可实时输出人类可读的决策依据（描述、元动作）。Doe-1 则走向另一个极端——统一生成式架构，将整个感知 - 控制回路转化为一个大型自回归模型。这一架构下，没有显式的分模块，VLM 的语言能力和世界模型的生成能力融为一体，在序列生成中同时完成对环境的理解和模拟。简单来说，VLM-E2E 是“端到端模型 + 离线 VLM 监督”，DriveVLM 是“传统栈 + 在线 VLM 助手”，Doe-1 是“端到端大模型（自带语言与视觉生成能力）”。

这种差异带来了系统复杂度与灵活性的不同。VLM-E2E 的融合方式对现有端到端方法改动最小，易于在已有框架上实现，只需增加一个文本分支训练即可。但它的语义信息是内嵌到模型权重的，推理时缺乏灵活的语言接口。相比之下，DriveVLM 通过模块化设计保留了显式语言接口：它可以输出场景描述、接受问询甚至与人对话解释决策。这为自动驾驶系统增加了交互和解释维度（例如 DriveVLM 可以对工程师解释“我为何这样做”）。Doe-1 则更像训练一个智能代理，其输出既包括行动也包括描述，天然具有解释性（模型总是先生成对当前场景的描述，再给出行动），并且能够接受不同 Prompt 完成不同任务。然而，Doe-1 的语言能力完全依赖训练语料（如 GPT-4 生成的 QA 对），远不如 DriveVLM 使用的预训练 LLM 那样强大。这意味着 DriveVLM 在涉及常识推理和自由语言方面潜力更大，而 Doe-1 受限于训练数据可能语言多样性和深度不足，其回答的丰富性和准确性在开放域问题上可能不及 DriveVLM。举例来说，如果询问系统“前方车辆突然停车可能是什么原因？”，DriveVLM 基于 LLM 或许会推理出多种可能（避让行人、发动机故障等）并用自然语言解释；而 Doe-1 仅在训练集中学过有限模式，可能无法给出超出驾驶直接相关范围的解释。

### 创新亮点比较

三者在各自工作中都有创新：VLM-E2E 的亮点在于提出将驾驶员注意力显式融入端到端训练。它解决了传统 E2E 模型缺乏对交通规则、关键语义考虑的痛点，通过文本监督引导模型关注红灯、行人等。尤其是引入动态模态权重和平衡视觉/文本信息的策略在多模态融合领域有通用意义，可避免某一模态信息淹没另一模态，提升融合效果。而 Doe-1 的最大创新是闭环世界模型思想：它成功将语言、视觉、控制三种模态的生成统一框架，这种将决策问题等价为序列生成问题的思路与 GPT 系列在 NLP 中的范式如出一辙。Doe-1 模型可以被视作“驾驶界的 GPT”，其突破在于证明了这种范式在自动驾驶时空决策问题上可行，并展现了执行感知、问答、规划多任务的潜力。DriveVLM 的创新则侧重系统集成和实用性：它首创性地将 LLM 的推理能力和传统驾驶模块融合，提出了“元动作 + 决策描述”的中间表示来桥接语言决策和数值控制 () ()。此外，DriveVLM 团队打造的 SUP-AD 数据集和评测方法，以及在真车上的验证，也是一种务实创新——他们不仅提出方法，还构建了从数据到评测的完整链条，推动了该方向研究的标准化。

在任务范围上，Doe-1 和 DriveVLM 都追求多任务通用：前者通过 Prompt 调度任务，一个模型多用，后者通过设计多个输出（描述、分析、元动作、轨迹）覆盖从感知到规划的各环节 ()。VLM-E2E 相对单一一些，主要着眼于提高规划性能，而未涉及主动生成描述或回答问题。这意味着在可解释 AI 方面，DriveVLM 和 Doe-1 更进一步——DriveVLM 能输出人类可读的决策依据，Doe-1 则内置描述生成，能够回答与环境相关的问题。VLM-E2E 虽然融合了文本特征，但这些特征只是嵌入在模型内部，不会以文本形式输出给用户，所以它提升的是模型的隐含认知，而非直接的解释性。

### 实验性能比较

在规划决策性能上，如果直接比较数值，DriveVLM-Dual 显然取得了最优成绩（nuScenes 上 3 秒误差 0.48 m, 碰撞率 0.10% ()）。但需注意比较的公平性：DriveVLM-Dual 借助高清地图、高精度检测等，具有先天优势。反观 VLM-E2E 和 Doe-1 均假设输入只有摄像头图像（和有限的高层导航指令），不依赖地图额外信息。因此，更合理的比较是看各自相对于同条件基线的提升：VLM-E2E 相对纯视觉端到端基线，实现了长距离误差约 5% 的降低（2.80 降至 2.68）和碰撞率超过 50% 的降低（2.34% 降至 1.17%）。Doe-1 相对只用前视相机的其他方法（如 OccNet 3 秒误差 2.99 m）将误差降低了约 65%（降至 1.07 m，在闭环评测下）。DriveVLM 则相对其基线 VAD-Base（已非常强大）继续将误差再降低 16%（0.37 降至 0.31） () ()。因此各自相对提升幅度看，VLM-E2E 和 Doe-1 对弱基线提升较大，DriveVLM 对强基线也有提升但幅度有限。不过，在绝对指标上，经典方法结合大模型（DriveVLM-Dual）的确效果最佳，说明传统几何模块的信息（如高精地图、动态轨迹）对精度帮助显著。这也暗示了一个重要结论：仅靠视觉和语言，目前的大模型方法尚难完全超越融合多传感器的传统方法，但它们已经展现出赶上的趋势。尤其 Doe-1，仅凭单目就几乎达到多传感器方法的水准。DriveVLM 若剥离地图等信息，其 DriveVLM 纯模型的结果（3 秒误差 0.68 m）相比 Doe-1（1.07 m）还是更好一些。这可能归功于 DriveVLM 利用了预训练 LLM 的强大先验，以及使用环视多摄像头图像，而 Doe-1 在这方面信息量不足。但 Doe-1 胜在统一学习，其表现有潜力随着模型和数据规模扩大而持续提升。

在感知与语义方面，VLM-E2E 通过辅助任务评估（BEV 分割、预测）证明了融合语义后的模型感知精度提升。DriveVLM 和 Doe-1 没有直接进行分割或检测评估，而是通过问答和描述任务侧面衡量感知效果。Doe-1 的场景描述 CIDEr 分数超过 OmniDrive-3D，表明其感知到的关键信息较全面。DriveVLM 的场景描述评分在 SUP-AD 也非常高（0.71 vs 次佳 0.49） ()。如果考虑可解释感知，DriveVLM 和 Doe-1 都能输出自然语言描述，这是 VLM-E2E 无法直接做到的。DriveVLM 甚至可在车上生成口头解释。因而，在人机交互友好性上，DriveVLM/Doe-1 > VLM-E2E。

### 局限性比较

VLM-E2E 的局限在于适用范围有限：它主要服务于提高规划性能，对于解释、仿真等其它用途未涉足。同时，它需要有标注数据来精炼 BLIP-2 的输出，在换域或无监督场景下难以直接应用。此外，其改进虽明显但仍不足以应对一些极端情况，比如当视觉严重受损时（大雾、强逆光），仅靠学到的语义注意力也许不足。相形之下，DriveVLM 因为保留了传统感知，所以在恶劣天气下至少还能依赖雷达、激光等，这方面更健壮。但 DriveVLM 的问题是复杂度较高：需要训练和维护一个大模型模块和传统模块，系统调试难度大。而 Doe-1 走极端端到端路线，未来若能充分训练可能简化系统，但现阶段训练成本和风险都最高。Doe-1 还可能面临长时间预测漂移的问题，即生成 10 秒以上长序列时，小误差累积成为大偏差，这在文中没有完全解决。因此在闭环仿真中，它或许需要定期用传感器真值重置，尚无法无限 rollout。DriveVLM 则避免了这个问题，因为真实传感器每时刻都矫正世界状态。

一个值得关注的共同挑战是：大模型的安全与验证。无论是融合型还是生成型，引入学习的大模型都带来了传统规则系统没有的不确定性和验证难度。VLM-E2E 虽然只在训练用到 VLM，但融合后模型内部机理更难解释，如何证明它在所有情况下都可靠还不清楚。DriveVLM 和 Doe-1 让模型输出人类可读的信息，一定程度缓解了黑箱问题，但仍需要建立完整的验证流程确保模型不会在关键时刻产生谬误建议或预测。在安全 -critical 的自动驾驶领域，这方面的要求会高于一般 AI 应用，需要更严格的测试和或许新的验证理论。

### 与既有文献的对比

在自动驾驶结合大模型方面，除 DriveVLM 外近期还有诸多探索，例如 OmniDrive、GPT-Driver、DriveGPT 等。相较而言，VLM-E2E、Doe-1、DriveVLM 三者各有独特之处：DriveVLM 的实车验证和性能提升使其在实用性上领先一步，而 OmniDrive 等偏重于框架提出，未必有实际部署。据报道，OmniDrive 也尝试了 3D Q-Former 将多模态输入接 LLM 并输出驾驶指令的方案。DriveVLM 与之相比引入了 Dual 混合，使之更贴近应用并取得更优成绩。Doe-1 与 DriveVLM 相比，则代表两种理念：一个是一体化大模型学习（更贴近学术对 AGI 的追求），一个是结合工程约束的混合智能体（更务实高效）。可以预见，这两条路线未来可能相互借鉴、融合。例如，Doe-1 的生成世界模型可用于 DriveVLM 的场景模拟和预测模块；DriveVLM 的 LLM 常识可用于指导 Doe-1 的描述生成或校验其预测。事实上，业界已有工作尝试结合 LLM 与世界模型，例如 DriveDreamer-2 利用 LLM 增强驾驶视频生成模型，以产生更多元的驾驶场景。相较这些早期尝试（主要在仿真中验证），Doe-1 和 DriveVLM 把这个思路推到了更完整的闭环和现实验证，显示出领先的效果。

综上所述，三篇论文各自在提升自动驾驶智能方面迈出了一步：VLM-E2E 证明了给端到端驾驶模型“补语义”能显著优化决策质量；Doe-1 开拓了用大型生成模型统一解决驾驶任务的新范式，展示了多任务一体化的可能；DriveVLM 则表明预训练的大模型可以与现有系统有机结合，立即提升复杂场景处理能力，并已有现实可行性。这些工作在架构上一个比一个更激进，但都朝着让自动驾驶系统更聪明、更安全、更可解释的方向前进。

## 展望：对未来 3-5 年的影响

融合 VLM 和世界模型的这些探索，预示了自动驾驶技术发展的新趋势：未来的自动驾驶“大脑”将不再仅仅依赖规则和小型神经网络，而会融合大规模预训练知识、模态交互以及环境模拟能力。具体而言：

- 更强的复杂场景理解： 未来 3-5 年，自动驾驶系统在处理施工区、事故现场、特殊天气等少见场景时，将得益于大模型的知识泛化能力。DriveVLM 已展示了大模型可以理解微妙的人类行为和复杂场景语境。随着这类模型参数更大、训练数据更多，其对长尾场景的“见识”将更丰富，推理也会更可靠（例如 Qwen2.5-VL 与 Phi-4-multimodel）。甚至可能将 Reasoning 模型扩展到多模态，成为视觉推理模型（例如 QvQ），进一步提升模型的思维能力。比如，一个经训练的大模型或能从交警手势、非常规标志中正确解读含义，这是传统方法难以覆盖的。人类驾驶依赖大量经验与常识，大模型正提供类似的经验库。在未来几年，我们可能会看到具有常识推理能力的驾驶 AI 出现：它知道在学校区域要格外小心、看到前车打开双闪可能预示故障等，这些都可以通过 VLM/LLM 的常识学习得到。这将提高自动驾驶在陌生城市和极端条件下的鲁棒性。
- 决策可解释性与人机交互： 由于自动驾驶涉及安全，其决策透明度非常重要。融合语言模型使系统可以用接近自然语言的方式解释自己。未来的自动驾驶汽车或许能实时地描述环境和自己的意图给乘客听。例如：“我减速是因为检测到前方有人行横道上有行人”——这类解释能够增强乘客对 AI 的信任，也便于开发者调试。DriveVLM 已在这方面做出了尝试，Dual 系统可以输出决策描述 ()。3-5 年内，随着法规要求和用户期望，解释型 AI 驾驶助手可能成为标配。乘客可以问车辆：“为什么刚才突然刹车？”，系统将通过 VLM 模块回答：“检测到前方车辆急停，我为保持安全距离而制动”，类似的情景将提升用户体验和安全信心。
- 闭环世界模型与仿真训练： Doe-1 展示的世界模型能够根据动作生成未来场景，这一能力在未来开发中用途广泛。首先，它可以用来构建闭环仿真环境：AI 可以在脑海中模拟不同决策的后果，再选择最优方案。这类似人类司机脑补“如果我现在并线结果如何”。未来的驾驶算法也许会在每次执行前，在内部快速 Rollout 几种方案，预估碰撞风险和通行效率，再做决定（需要极高计算效率，但有希望实现）。其次，世界模型还可用于数据增强和仿真训练。自动驾驶真实数据采集昂贵，而一个强大的生成世界模型可模拟逼真的驾驶视频和事件，为训练提供“无限”样本。DriveVLM 等需要大量训练数据的大模型，可借助这种生成数据弥补长尾分布，提升模型性能。特别在安全测试上，逼真的仿真可以覆盖极端情况，大模型生成的场景也许比现有游戏引擎更丰富多变，因为它可以基于真实数据学习。未来几年，我们可能看到大模型驱动的自动驾驶模拟器出现，用于验证算法和训练决策网络，其场景多样性和真实性远超现有模拟。
- 混合规划范式： 由 DriveVLM 引领的混合规划思路可能成为主流。即高层决策由大模型提供灵活智能，低层控制由可靠算法保障。这样的双层结构符合逐步提升信任的工程路径。在 3-5 年内完全用端到端大模型接管驾驶尚有困难（主要是可靠性和监管问题），因此人类指导 +AI 决策 + 规则兜底的架构会是过渡期的理想方案。比如，一辆车的高级 AI 负责看懂复杂场景和制定战术（如何并线超车），然后将目标交给低级控制去精确实现。这类似航空领域的飞行管理系统与自动驾驶仪的关系。混合范式还能方便地融入人工干预：在少数极端状况下，人类远程监控可直接介入或通过简短指令指导大模型决策，保障安全。
- 模型高效化与定制化： 目前这些大模型都面临计算量大的问题，因此未来研究重点之一是模型压缩和专用芯片。在 3-5 年内，可能会出现针对自动驾驶优化的中型视觉语言模型（参数量适中但保留关键常识能力），或者通过蒸馏技术将大模型知识提炼到小模型中用于部署。VLM-E2E 其实提供了一种蒸馏思路：用大模型生成的注意力线索来训练一个小模型。未来或许可以用更广泛的大模型知识（不仅注意力，还有交规常识等）去监督小模型训练。这样既可享受大模型好处，又保证实时性能。硬件方面，自动驾驶域对算力需求高，或许会推动出现车载 AI 加速芯片，专门加速 Transformer 这类模型的推理，使在车端跑百亿参数模型成为可能。3-5 年内，高端自动驾驶汽车可能配备数百 TOPS 甚至 POPS 级别算力的 AI 模块，足以部署一个经过剪裁的大型驾驶模型实时运行。
- 安全与标准： 这些工作的出现也将促使自动驾驶 AI 安全标准的演进。监管者可能要求对大模型驱动决策进行专门评估，例如验证模型不会产生危险幻觉。行业或许会制定测试题库，用语言和场景相结合的形式考核 AI 驾驶员，如提问“如果前方车辆突然变道你怎么办”并查看模型回答。这类似驾驶员笔试 + 路考结合，只不过对象是 AI。VLM 融合架构的出现恰好提供了接口让 AI 参与这种测试（AI 可以用自然语言解释自己的应对）。因此未来标准中，自动驾驶 AI 能否正确回答驾驶相关问题、能否在模拟环境中表现出安全驾驶行为，可能成为认证的一部分。总之，大模型的引入迫使我们从认知智能层面来审视自动驾驶系统，而不仅是感知和控制的正确性。

总结：VLM-E2E、Doe-1、DriveVLM 等工作的持续进展，正引领自动驾驶从“感知 - 规划 - 控制”的传统范式迈向“认知 - 推理 - 行动”的新阶段。在未来 3-5 年，我们预计视觉语言大模型将深度融入自动驾驶：短期内以辅助决策和增强安全为主（如 DriveVLM-Dual 这样的形式），中长期则有望发展出高度自主的驾驶智能体。尽管仍有诸多挑战，如模型可靠性、实时性和法律责任划分等需要解决，但这些研究已证明大模型有潜力让自动驾驶车辆变得更聪明、更安全。当车辆能像经验丰富的司机那样理解周围世界并预测他人意图时，真正的无人驾驶将更近一步。正如 Doe-1 论文标题所隐含的那样：我们正在迈向大型驾驶世界模型时代，在这个过程中，VLM 和世界模型的融合将扮演关键角色，为自动驾驶技术注入前所未有的活力和智慧。

## 参考材料

- [VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion](https://arxiv.org/html/2502.18042)
- [Doe-1: Closed-Loop Autonomous Driving with Large World Model](https://ar5iv.org/html/2412.09627)
- [DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/abs/2402.12289)
